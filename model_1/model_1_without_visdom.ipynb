{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from masked_cross_entropy import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import io\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import visdom\n",
    "vis = visdom.Visdom()\n",
    "%matplotlib inline\n",
    "\n",
    "USE_CUDA = False\n",
    "\n",
    "MIN_LENGTH = 5\n",
    "MAX_LENGTH = 32\n",
    "\n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        length = Variable(torch.LongTensor(length)).cuda()\n",
    "    else:\n",
    "        length = Variable(torch.LongTensor(length))\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = functional.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3 # Count default tokens\n",
    "\n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed: return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        keep_words = []\n",
    "        \n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words %s / %s = %.4f' % (\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.index_word(word)\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z,.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def read_langs(lang1, lang2, term=\"txt\", reverse=False, normalize=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "\n",
    "    # Attach the path here for the source and target language dataset\n",
    "    #filename = '%s-%s.%s' % (lang1, lang2, term)\n",
    "    \n",
    "    # Short data:\n",
    "    filename = \"en-fr_short.bpe2bpe\";\n",
    "    \n",
    "    # This creats the file directory name whichis used below\n",
    "\n",
    "    # lines contains the data in form of a list \n",
    "    lines = open(filename, encoding=\"utf8\").read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    if normalize == True:\n",
    "        pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    else: \n",
    "        pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def filter_pairs(pairs, MIN_LENGTH, MAX_LENGTH):\n",
    "    filtered_pairs = []\n",
    "    for pair in pairs:\n",
    "        if len(pair[0]) >= MIN_LENGTH and len(pair[0]) <= MAX_LENGTH \\\n",
    "            and len(pair[1]) >= MIN_LENGTH and len(pair[1]) <= MAX_LENGTH:\n",
    "                filtered_pairs.append(pair)\n",
    "    return filtered_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "\n",
    "    # Get the source and target language class objects and the pairs (x_t, y_t)\n",
    "    \n",
    "    ## 1. normalize in argument ????\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, \"bpe2bpe\",reverse, False)\n",
    "    print(\"Read %d sentence pairs\" % len(pairs))\n",
    "    \n",
    "    ## 2. MIN LENGTH & MAX LENGTH ????\n",
    "    pairs = filter_pairs(pairs, MIN_LENGTH, MAX_LENGTH)\n",
    "    print(\"Filtered to %d pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "    \n",
    "    print('Indexed %d words in input language, %d words in output' % (input_lang.n_words, output_lang.n_words))\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "# Return a list of indexes, one for each word in the sentence, plus EOS\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "# Pad a with the PAD symbol\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq\n",
    "\n",
    "\n",
    "def random_batch(batch_size):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "\n",
    "    # Choose random pairs\n",
    "    for i in range(batch_size):\n",
    "        pair = random.choice(pairs)\n",
    "        input_seqs.append(indexes_from_sentence(input_lang, pair[0]))\n",
    "        target_seqs.append(indexes_from_sentence(output_lang, pair[1]))\n",
    "\n",
    "    # Zip into pairs, sort by length (descending), unzip\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    \n",
    "    # For input and target sequences, get array of lengths and pad with 0s to max length\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "    # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n",
    "    input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "    target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_var = input_var.cuda()\n",
    "        target_var = target_var.cuda()\n",
    "        \n",
    "    return input_var, input_lengths, target_var, target_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 50300 sentence pairs\n",
      "Filtered to 2503 pairs\n",
      "Indexing words...\n",
      "Indexed 2192 words in input language, 2483 words in output\n",
      "keep_words 266 / 2189 = 0.1215\n",
      "keep_words 285 / 2480 = 0.1149\n",
      "Trimmed from 2503 pairs to 639, 0.2553 of total\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data('en', 'fr', False)\n",
    "\n",
    "# TRIMMING DATA:\n",
    "# Trimming is optional but could be done to reduce the data size and make processing faster\n",
    "# Removes words with frequency < 5\n",
    "\n",
    "MIN_COUNT = 5\n",
    "\n",
    "input_lang.trim(MIN_COUNT)\n",
    "output_lang.trim(MIN_COUNT)\n",
    "\n",
    "\n",
    "keep_pairs = []\n",
    "\n",
    "for pair in pairs:\n",
    "    input_sentence = pair[0]\n",
    "    output_sentence = pair[1]\n",
    "    keep_input = True\n",
    "    keep_output = True\n",
    "\n",
    "    for word in input_sentence.split(' '):\n",
    "        if word not in input_lang.word2index:\n",
    "            keep_input = False\n",
    "            break\n",
    "\n",
    "    for word in output_sentence.split(' '):\n",
    "        if word not in output_lang.word2index:\n",
    "            keep_output = False\n",
    "            break\n",
    "\n",
    "    # Remove if pair doesn't match input and output conditions\n",
    "    if keep_input and keep_output:\n",
    "        keep_pairs.append(pair)\n",
    "\n",
    "print(\"Trimmed from %d pairs to %d, %.4f of total\" % (len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "pairs = keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size #no of words in the input Language\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None): # hidden vector starts with zero (a guess!)\n",
    "        \n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        embedded = self.embedding(input_seqs) # size = (max_length, batch_size, embed_size). NOTE: embed_size = hidden size here\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) # size = (max_length * batch_size, embed_size)\n",
    "        \n",
    "        outputs, hidden = self.gru(packed, hidden) # outputs are supposed to be probability distribution right?\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        return outputs, hidden\n",
    "\n",
    "    #output (seq_len, batch, hidden_size * num_directions): tensor containing the output features h_t from the last layer of the \n",
    "    # RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.\n",
    "    \n",
    "    # h_n (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t=seq_len\n",
    "    \n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "\n",
    "        if USE_CUDA:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                #attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "                attn_energies[b, i] = self.score(hidden[b,:], encoder_outputs[i, b])\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        #BUG: print(F.softmax(attn_energies).unsqueeze(0).size()) ?\n",
    "        return F.softmax(attn_energies).unsqueeze(1)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            #print(torch.cat((hidden, encoder_output), 1), attn)\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 0))\n",
    "            #energy = self.v.dot(energy)\n",
    "            #print(self.v.squeeze(0), energy)\n",
    "            energy = (self.v.squeeze(0)).dot(energy)\n",
    "            return energy\n",
    "        \n",
    "##############################################################################################\n",
    "#########################  BAHDANAU_ATTN_DECODER_RNN  ########################################\n",
    "##############################################################################################\n",
    "        \n",
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        ## 3. self.max_length = max_length\n",
    "        self.max_length = MAX_LENGTH\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn('concat', hidden_size)\n",
    "        \n",
    "        # Modifications made below in 2 lines\n",
    "        self.gru = nn.GRU(2*hidden_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        #self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        # TODO: FIX BATCHING\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, word_input.data.shape[0], -1) # S=1 x B x N , ## N = hidden size (doubt)\n",
    "        #print(word_embedded.size())\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        context = context.transpose(0, 1) # 1 x B x N\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2) # 1 x B x 2N (There seems to be a mistake here)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden) # 1 x B x h , n_layers*directions x B x h\n",
    "\n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        #output = F.log_softmax(self.out(output))\n",
    "        \n",
    "        ## Modification made below too\n",
    "        #output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context.squeeze(0)), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        \n",
    "        # print(self.output.size(), output.size(), hidden.size(), attn_weights.size())\n",
    "        # 330, torch.Size([80, 330]) torch.Size([2, 80, 1024]) torch.Size([80, 1, 10])\n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEAM SEARCH:\n",
    "    # A dictinary is maintained which keeps track of the sequence within top first k probabilities\n",
    "    # If k = 1, it is same as greedy search. \n",
    "    # The dictinary is of the form {sequence: [probability, decoder_hidden, decoder_attention] }\n",
    "    # The dictinary data type is  {string: [int, torch.Size([num_directions, batch_size, hidden_size]), torch.Size([max_length+1, max_length+1])]}\n",
    "\n",
    "\n",
    "def update_dictionary(target_sequence, topv, topi, key, dec_hidden, decoder_attns):\n",
    "    if len(target_sequence) == 0:\n",
    "        for i in range(len(topi)):\n",
    "            target_sequence.update({str(topi[i]) : [topv[i], dec_hidden, decoder_attns] })\n",
    "            print()\n",
    "    else:\n",
    "        prev_val = target_sequence[key][0]\n",
    "        for i in range(len(topi)):\n",
    "            target_sequence.update({key+\"-\"+str(topi[i]) : [topv[i]*prev_val, dec_hidden, decoder_attns] })\n",
    "        del[target_sequence[key]]\n",
    "        \n",
    "\n",
    "def get_seq_through_beam_search(max_length, decoder, decoder_input, decoder_hidden, decoder_attentions, encoder_outputs, kmax ):\n",
    "    \n",
    "    target_sequence = dict()\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        \n",
    "        if di == 0:\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs )\n",
    "            topv, topi = decoder_output.data.topk(kmax)\n",
    "            #topv = topv[0].numpy()\n",
    "            topv = np.exp(topv[0].numpy())\n",
    "            topi = topi[0].numpy()\n",
    "#             print(topi)\n",
    "#             print(topv)\n",
    "            decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "            update_dictionary(target_sequence, topv, topi, None, decoder_hidden, decoder_attentions)\n",
    "        else:\n",
    "            temp = target_sequence.copy()\n",
    "            keys = list(temp.keys())\n",
    "            for i in range(len(keys)):\n",
    "                inp = int(keys[i].split(\"-\")[-1] if len(keys[i]) > 1 else keys[i])\n",
    "                if inp != EOS_token:\n",
    "                    dec_input = Variable(torch.LongTensor([inp]))\n",
    "                    decoder_output, dec_hidden, decoder_attention = decoder( dec_input, temp[keys[i]][1], encoder_outputs )\n",
    "                    topv, topi = decoder_output.data.topk(kmax)\n",
    "                    #topv = topv[0].numpy()\n",
    "                    topv = np.exp(topv[0].numpy())\n",
    "                    topi = topi[0].numpy()\n",
    "                    dec_attns = temp[keys[i]][2]\n",
    "                    dec_attns[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "                    update_dictionary(target_sequence, topv, topi, keys[i], dec_hidden, dec_attns)\n",
    "        \n",
    "        # Sort the target_Sequence dictionary and keep top k sequences only\n",
    "        target_sequence = dict(sorted(target_sequence.items(), key=lambda x: x[1][0], reverse=True)[:kmax])\n",
    "#         kk = list(target_sequence.keys())\n",
    "#         print()\n",
    "#         print(len(kk))\n",
    "#         for i in range(len(kk)):\n",
    "#             print(kk[i], \":\", target_sequence[kk[i]][0])\n",
    "        \n",
    "#         # Break if the sentence with EOS has highest prob\n",
    "#         temp_key = int(sorted(target_sequence.items(), key=lambda x: x[1][0], reverse=True)[:1][0][0][-1])\n",
    "#         if (temp_key == EOS_token):\n",
    "#             break\n",
    "     \n",
    "    # Get the sequence, decoder_attentions with maximum probability\n",
    "    pair = sorted(target_sequence.items(), key=lambda x: x[1][0], reverse=True)[:1][0]\n",
    "    seq = pair[0]\n",
    "    decoder_attentions = pair[1][2]\n",
    "    \n",
    "    # Get the decoded words:\n",
    "    decoded_words_indices = seq.split(\"-\")\n",
    "    decoded_words = [output_lang.index2word[int(i)] for i in decoded_words_indices]\n",
    "    if int(decoded_words_indices[-1]) != EOS_token:\n",
    "        decoded_words.append('<EOS>')\n",
    "    \n",
    "    return decoded_words, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))\n",
    "\n",
    "# Evaluation is mostly the same as training, but there are no targets. Instead we always feed the decoder's predictions back to itself. \n",
    "# Every time it predicts a word, we add it to the output string. If it predicts the EOS token we stop there. We also store the decoder's attention outputs for each step to display later.\n",
    "\n",
    "def evaluate(input_seq, max_length=MAX_LENGTH):\n",
    "    input_lengths = [len(input_seq)]\n",
    "    input_seqs = [indexes_from_sentence(input_lang, input_seq)]\n",
    "    input_batches = Variable(torch.LongTensor(input_seqs), volatile=True).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_batches = input_batches.cuda()\n",
    "        \n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)    \n",
    "    \n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]), volatile=True) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "            \n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "    kmax = 5\n",
    "    decoded_words, decoder_attentions = get_seq_through_beam_search(max_length, decoder, decoder_input, decoder_hidden, decoder_attentions, encoder_outputs, kmax )\n",
    "\n",
    "    #decoded_words, decoder_attentions = get_seq_through_beam_search(len(input_seq), decoder, decoder_input, decoder_hidden, decoder_attentions, encoder_outputs, kmax )\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:len(decoded_words)+1, :len(encoder_outputs)]\n",
    "\n",
    "\n",
    "# We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:\n",
    "def evaluate_randomly():\n",
    "    [input_sentence, target_sentence] = random.choice(pairs)\n",
    "    evaluate_and_show_attention(input_sentence, target_sentence)\n",
    "\n",
    "def evaluate_and_show_attention(input_sentence, target_sentence=None):\n",
    "    output_words, attentions = evaluate(input_sentence)\n",
    "    \n",
    "    # Calculating the bleu score excluding the last word (<EOS>)\n",
    "    #bleu_score = nltk.translate.bleu_score.sentence_bleu([target_sentence], ' '.join(output_words[:-1]))\n",
    "    \n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', input_sentence)\n",
    "    if target_sentence is not None:\n",
    "        print('=', target_sentence)\n",
    "    print('<', output_sentence)\n",
    "    #print(\"BLUE SCORE IS:\", bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(input_batches, input_lengths, target_batches, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, max_length=MAX_LENGTH):\n",
    "    \n",
    "    # Added 2 lines below\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size))\n",
    "    \n",
    "    # Q.) Why do we use the last hidden state for t = 0. is of because z_t = phi_init(context_vector)\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size))\n",
    "\n",
    "    # Move new Variables to CUDA\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "    # Loss calculation and backpropagation\n",
    "    loss = masked_cross_entropy(\n",
    "        all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_lengths\n",
    "    )\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0], ec, dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:37: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.3. Note that arange generates values in [start; end), not [start; end].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3m 55s (- 74m 25s) (2 5%) 5.2162\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Here &apos;s why .\n",
      "= Voici pourquoi .\n",
      "< EOS\n",
      "8m 10s (- 73m 30s) (4 10%) 3.6973\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Thank you .\n",
      "= Merci .\n",
      "< Merci . EOS\n",
      "12m 0s (- 68m 4s) (6 15%) 3.1572\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Thank you .\n",
      "= Merci .\n",
      "< Merci . EOS\n",
      "16m 39s (- 66m 37s) (8 20%) 3.0965\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Thank you .\n",
      "= Merci .\n",
      "< Merci . EOS\n",
      "20m 31s (- 61m 34s) (10 25%) 2.4959\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> What happened ?\n",
      "= Que s&apos; est-il passé ?\n",
      "< ? EOS\n",
      "24m 22s (- 56m 52s) (12 30%) 2.4154\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Why ?\n",
      "= Pourquoi ?\n",
      "< Pourquoi ? EOS\n",
      "61m 13s (- 113m 43s) (14 35%) 2.2073\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> So here we are .\n",
      "= Et nous y voilà .\n",
      "< Merci . EOS\n",
      "129m 24s (- 194m 6s) (16 40%) 2.1091\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Thank you .\n",
      "= Merci .\n",
      "< Merci . EOS\n",
      "133m 28s (- 163m 7s) (18 45%) 1.8250\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Thank you .\n",
      "= Merci .\n",
      "< Merci . EOS\n",
      "137m 13s (- 137m 13s) (20 50%) 1.9396\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> And guess what ?\n",
      "= Et vous savez quoi ?\n",
      "< Et ? EOS\n",
      "141m 38s (- 115m 53s) (22 55%) 1.7753\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> All right .\n",
      "= Très bien .\n",
      "< Merci . EOS\n",
      "145m 43s (- 97m 9s) (24 60%) 1.5851\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> &quot; No &quot; .\n",
      "= &quot; Non &quot; .\n",
      "< Merci . EOS\n",
      "150m 2s (- 80m 47s) (26 65%) 1.6141\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> We can do it !\n",
      "= On peut le faire !\n",
      "< Oui . EOS\n",
      "156m 38s (- 67m 8s) (28 70%) 1.5671\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> All right .\n",
      "= Bien .\n",
      "< Merci . EOS\n",
      "159m 42s (- 53m 14s) (30 75%) 1.3875\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Thank you very much .\n",
      "= Merci beaucoup .\n",
      "< Merci beaucoup . EOS\n",
      "164m 3s (- 41m 0s) (32 80%) 1.1022\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> So what should we do now ?\n",
      "= Alors que faire maintenant ?\n",
      "< Alors que faire ? EOS\n",
      "167m 56s (- 29m 38s) (34 85%) 1.2165\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Thank you very much .\n",
      "= Merci beaucoup .\n",
      "< Merci beaucoup . EOS\n",
      "171m 59s (- 19m 6s) (36 90%) 1.1211\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> &quot; Not good &quot; .\n",
      "= &quot; Pas bon ! &quot;\n",
      "< Oui . EOS\n",
      "176m 58s (- 9m 18s) (38 95%) 0.9689\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Thank you .\n",
      "= Merci .\n",
      "< Merci . EOS\n",
      "181m 24s (- 0m 0s) (40 100%) 1.0196\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "> Thank you .\n",
      "= Merci .\n",
      "< Merci . EOS\n"
     ]
    }
   ],
   "source": [
    "# Configure models\n",
    "# attn_model = 'dot'\n",
    "hidden_size = 1024\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 80\n",
    "# batch_size = 50\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "clip = 1.0\n",
    "#clip = 1.0 # Based on our paper, clipping gradient norm is 1\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "#n_epochs = 50000\n",
    "n_epochs = 40\n",
    "epoch = 0\n",
    "#plot_every = 20\n",
    "#print_every = 100\n",
    "#evaluate_every = 10000 # We check the validation in every 10,000 minibatches\n",
    "\n",
    "plot_every = 2\n",
    "print_every = 2\n",
    "evaluate_every = 2 # We check the validation in every 10,000 minibatches\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers, dropout=dropout)\n",
    "decoder = BahdanauAttnDecoderRNN( hidden_size, output_lang.n_words, n_layers, dropout_p=dropout)\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "######                         PART-III : Modeling                   #####\n",
    "##########################################################################\n",
    "\n",
    "ecs = []\n",
    "dcs = []\n",
    "eca = 0\n",
    "dca = 0\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    epoch += 1\n",
    "\n",
    "    # Get training data for this cycle\n",
    "    input_batches, input_lengths, target_batches, target_lengths = random_batch(batch_size)\n",
    "    #print(input_batches.size())\n",
    "    \n",
    "    # Run the train function\n",
    "    loss, ec, dc = train(\n",
    "        input_batches, input_lengths, target_batches, target_lengths,\n",
    "        encoder, decoder,\n",
    "        encoder_optimizer, decoder_optimizer)\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "    eca += ec\n",
    "    dca += dc\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "\n",
    "    if epoch % evaluate_every == 0:\n",
    "        evaluate_randomly()\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "        eca = 0\n",
    "        dca = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1e6d98ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npSZBREREkvr/sJYrXrDM2fUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1e6db7828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
