{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "-1.0621 -0.3288  1.1981  0.2501 -1.6450  0.2469 -0.8459 -1.3823\n",
      "-0.4694  1.9801  1.6282 -1.3543  0.2387  1.7283 -0.7127 -0.5819\n",
      " 0.5085  2.1128 -1.7126 -1.2009 -0.5804 -1.6153 -0.0525 -0.1961\n",
      "-0.5935 -0.3429 -0.2120  1.9721  0.2144 -1.2680 -1.0744  1.3904\n",
      "-0.4951 -0.6379  0.3515  1.2364 -0.0588 -0.7529 -0.7056  2.1758\n",
      "-1.9269  0.1559  1.7837  1.1505  0.8843 -1.3509  0.4502 -0.7376\n",
      "-2.3874 -0.9906  0.8133  0.2321 -1.5676  0.4423 -0.6881 -1.1063\n",
      " 0.3793  1.0939 -0.7279  0.8056 -0.7481 -0.1250  1.6187 -0.4917\n",
      " 0.1322  0.3280  0.2431 -1.0077  0.4445 -1.5049 -1.6693 -1.7160\n",
      " 1.1499  1.3409  0.4392  0.1849  2.5987 -1.4353 -0.2236  0.6242\n",
      "[torch.FloatTensor of size 10x8]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -1.0621 -0.3288  1.1981  0.2501 -1.6450  0.2469 -0.8459 -1.3823\n",
      " -0.4694  1.9801  1.6282 -1.3543  0.2387  1.7283 -0.7127 -0.5819\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.4694  1.9801  1.6282 -1.3543  0.2387  1.7283 -0.7127 -0.5819\n",
      "  0.5085  2.1128 -1.7126 -1.2009 -0.5804 -1.6153 -0.0525 -0.1961\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.5085  2.1128 -1.7126 -1.2009 -0.5804 -1.6153 -0.0525 -0.1961\n",
      " -0.5935 -0.3429 -0.2120  1.9721  0.2144 -1.2680 -1.0744  1.3904\n",
      "[torch.FloatTensor of size 3x2x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from masked_cross_entropy import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "emb = nn.Embedding(10, 8)\n",
    "print(emb.weight)\n",
    "input_lengths = [3,3]\n",
    "inp_seq = Variable(torch.LongTensor([[0,1],[1,2],[2,3]])) # max_length = 3, batch size = 2 (basically 2 sentences of size 3 each)\n",
    "embedded = emb(inp_seq)\n",
    "print(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "packed\n",
    "gru = nn.GRU(8,8,1,bidirectional=True)\n",
    "outputs, hidden = gru(packed, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.3887  0.2049 -0.1395  0.2054  0.0556 -0.0030 -0.2052  0.1519  0.1865\n",
      " -0.2024 -0.1286 -0.4030 -0.0180  0.2031 -0.2357  0.4924  0.4134  0.1953\n",
      "\n",
      "Columns 9 to 15 \n",
      "   0.1340  0.3503 -0.2416  0.1559 -0.0524 -0.3081 -0.1229\n",
      "  0.1530  0.5486 -0.6176  0.2426 -0.1266 -0.2613  0.0365\n",
      "\n",
      "(1 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.3909 -0.0164 -0.4796  0.1508  0.2107 -0.2635  0.4387  0.4684  0.2760\n",
      " -0.2676  0.0696 -0.1210  0.1447  0.1245  0.6422  0.5266 -0.2010 -0.0368\n",
      "\n",
      "Columns 9 to 15 \n",
      "   0.1327  0.4902 -0.6126  0.2420 -0.0181 -0.1562  0.0288\n",
      "  0.4322  0.2652 -0.4692 -0.3856 -0.3003 -0.0333 -0.1828\n",
      "\n",
      "(2 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.3636  0.1512 -0.1533  0.2590  0.1185  0.6338  0.5081 -0.2449  0.1379\n",
      " -0.2681  0.3872 -0.2309  0.3077  0.2475  0.4564 -0.1219 -0.1519 -0.3153\n",
      "\n",
      "Columns 9 to 15 \n",
      "   0.4016  0.0483 -0.4449 -0.4408 -0.1412  0.1036 -0.1091\n",
      "  0.0497  0.3328  0.0650  0.0550 -0.1937 -0.1735 -0.2291\n",
      "[torch.FloatTensor of size 3x2x16]\n",
      " \n",
      "\n",
      "\n",
      " Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.3636  0.1512 -0.1533  0.2590  0.1185  0.6338  0.5081 -0.2449\n",
      " -0.2681  0.3872 -0.2309  0.3077  0.2475  0.4564 -0.1219 -0.1519\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.1865  0.1340  0.3503 -0.2416  0.1559 -0.0524 -0.3081 -0.1229\n",
      "  0.1953  0.1530  0.5486 -0.6176  0.2426 -0.1266 -0.2613  0.0365\n",
      "[torch.FloatTensor of size 2x2x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "print(outputs, \"\\n\\n\\n\", hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.2681  0.3872 -0.2309  0.3077  0.2475  0.4564 -0.1219 -0.1519\n",
       " 0.1953  0.1530  0.5486 -0.6176  0.2426 -0.1266 -0.2613  0.0365\n",
       "[torch.FloatTensor of size 2x8]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden = hidden[:,1]\n",
    "encoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  1.1649  0.6227  0.0377  1.5082  1.2079 -1.0263  0.9527 -1.0573\n",
       "  0.9981  1.2160 -1.6219  1.4166  0.4511  0.2284  0.1482  0.9703\n",
       " -0.0821 -1.9819  1.2165  0.6273  1.2947 -0.1499 -0.3012  1.3003\n",
       "  0.1721  1.6277 -0.9819  0.2184 -1.3980  0.9430  0.4502  0.8252\n",
       " -0.0762 -0.8091  0.3428  0.8362  0.1281 -1.2115 -0.6643 -0.2419\n",
       "  0.2324  0.2410 -0.1980  0.4796 -0.3987  0.4532  0.7103 -1.4057\n",
       "  0.0390 -1.3084 -1.6195  0.9308  0.4632 -1.2675 -0.5356 -0.1859\n",
       "  0.9190  0.1293 -0.5696 -1.1240  1.0174 -1.9136  1.7826  0.6272\n",
       "  0.4495  0.1624  1.4156 -0.1969 -0.8393  0.2471  0.2182  0.7933\n",
       " -1.5586 -0.5604 -0.1043 -0.7584  0.8421 -0.3171 -0.3432  0.6025\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.2929 -0.8798  1.0703  0.7293  0.0651  0.2613  0.1121  2.6345\n",
       " -0.1835  0.8912  0.8478 -0.9610 -0.2272  0.8357 -0.7404 -1.1439\n",
       " -0.6491  0.6822 -0.0611  0.6637  1.1973  0.0192  0.3271  0.1196\n",
       " -1.0026  0.4247 -0.1238 -1.5490 -0.4546  1.7611 -0.3966 -0.2024\n",
       " -0.4742  1.0550  1.0576  0.2324 -1.3139  1.7602 -0.2670  1.8933\n",
       "  0.2384  0.9727  1.0545  0.9280  0.0927 -0.6767 -0.3525 -1.7695\n",
       "  1.2195  0.4057 -0.1918 -0.6779  1.8902  0.3258 -0.3257 -0.3383\n",
       "  0.5474 -0.8583 -1.3594 -0.0820 -0.9309  0.1814  0.2979 -0.4529\n",
       "  0.4688  0.2132 -1.4903  1.2026 -0.8915 -1.8452  0.4214 -0.4724\n",
       "  0.9557 -2.8247  1.0293 -0.0298 -0.2931 -0.7019  0.0668  0.3094\n",
       "\n",
       "(2 ,.,.) = \n",
       " -0.3338 -0.5624  0.8766  0.2272  2.0672  0.4941 -1.0691  0.2776\n",
       " -0.7743  0.4216  0.3747  1.5347 -2.4839 -0.3157 -0.1874  0.5080\n",
       "  0.6666  1.4319  1.2954 -2.8630  0.9074  0.6898  0.0426 -0.7078\n",
       "  1.0037 -1.0041  0.8411 -0.3943 -0.3429 -1.4265 -0.2624  0.5386\n",
       "  0.4820 -0.2425  0.6426 -0.4372  0.3277  0.6276 -0.6072 -1.1526\n",
       "  0.0174  0.8178 -0.4913  0.8533 -0.3414 -0.7619 -2.0308 -0.4376\n",
       " -1.1430  1.8485  0.6868  0.4284 -0.1484 -0.2081  2.7854  0.5487\n",
       " -0.4436  0.4887 -0.2196  0.2945  2.2723 -2.5471  1.6002 -1.7859\n",
       "  0.8507  0.8010 -0.1432 -0.9889 -0.7716 -1.4742  1.3236 -0.6491\n",
       "  0.2315 -1.9175 -1.3001 -0.2595 -0.4661  1.6412 -0.3644  0.5422\n",
       "[torch.FloatTensor of size 3x10x8]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 10, 8) # (num_layers*num_directions, batch, hidden_size)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn = nn.Linear(8 * 2, 8)\n",
    "v = nn.Parameter(torch.FloatTensor(1, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.1200  0.1529  0.5433  0.1439 -0.3919  0.3840  0.3570  0.6396\n",
       "-0.1981  0.4209  0.0659  0.0410  0.3421 -0.1809 -0.0197 -0.3295\n",
       "[torch.FloatTensor of size 2x8]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.1200\n",
       " 0.1529\n",
       " 0.5433\n",
       " 0.1439\n",
       "-0.3919\n",
       " 0.3840\n",
       " 0.3570\n",
       " 0.6396\n",
       "[torch.FloatTensor of size 8]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hid = encoder_hidden[0,:]\n",
    "hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.0613\n",
       "-0.5589\n",
       "-1.9943\n",
       " 1.0971\n",
       " 0.1027\n",
       " 1.4603\n",
       " 0.2676\n",
       "-0.6679\n",
       "[torch.FloatTensor of size 8]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out = x[0,0]\n",
    "enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.1200\n",
       " 0.1529\n",
       " 0.5433\n",
       " 0.1439\n",
       "-0.3919\n",
       " 0.3840\n",
       " 0.3570\n",
       " 0.6396\n",
       " 1.0613\n",
       "-0.5589\n",
       "-1.9943\n",
       " 1.0971\n",
       " 0.1027\n",
       " 1.4603\n",
       " 0.2676\n",
       "-0.6679\n",
       "[torch.FloatTensor of size 16]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cated = torch.cat((hid, enc_out), 0)\n",
    "cated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.7114e+18\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(v.squeeze(0)).dot(attn(cated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3068\n",
       " 0.6808\n",
       " 0.0503\n",
       "-0.7791\n",
       " 0.4188\n",
       "-0.2608\n",
       "-0.0225\n",
       " 0.8814\n",
       "[torch.FloatTensor of size 8]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(cated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 1.4427 -1.8166  0.5621 -0.8192  1.8608  1.6298 -2.0861  1.0015  0.0954 -0.9956\n",
       "\n",
       "Columns 10 to 14 \n",
       " 0.0110  0.7718  0.6203  0.5950  1.0942\n",
       "[torch.FloatTensor of size 1x15]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = Variable(torch.randn(1,15))\n",
    "topv, topi = temp.data.topk(5)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  5,  0, 14,  7], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topi[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam search trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The vocab size = 10 (assume)\n",
    "vocab_size = 20\n",
    "k = 10\n",
    "index_of_start = 0\n",
    "index_of_end = 19\n",
    "\n",
    "scores = torch.FloatTensor(vocab_size).zero_()\n",
    "scores\n",
    "\n",
    "prevKs = []\n",
    "nextYs = [torch.LongTensor(vocab_size).fill_(0)]\n",
    "nextYs[0][0] = index_of_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_dictionary(target_sequence, topv, topi, key, dec_hidden):\n",
    "    if len(target_sequence) == 0:\n",
    "        for i in range(topi):\n",
    "            target_sequence.update({str(topi[i]) : [topv[i], dec_hidden] })\n",
    "        else:\n",
    "            prev_val = target_sequence[key][0]\n",
    "            target_sequence.update({key+\"-\"+str(topi[i]) : [topv[i]*prev_val, dec_hidden] })\n",
    "            \n",
    "kmax = 15\n",
    "    \n",
    "# Run through decoder\n",
    "target_sequence = dict()\n",
    "\n",
    "for di in range(max_length):\n",
    "\n",
    "    if di == 0:\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs )\n",
    "        topv, topi = decoder_output.data.topk(kmax)\n",
    "        topv = topv[0].numpy()\n",
    "        topi = topi[0].numpy()\n",
    "        update_dictionary(target_sequence, topv, topi, None, decoder_hidden)\n",
    "    else:\n",
    "        temp = target_sequence.copy()\n",
    "        keys = list(temp.keys())\n",
    "        for i in range(len(keys)):\n",
    "            inp = keys[i].split(\"-\")[-1] if len(x) > 1 else keys[i]\n",
    "            if inp != EOS_token:\n",
    "                dec_input = Variable(torch.LongTensor([inp]))\n",
    "                decoder_output, dec_hidden, decoder_attention = decoder( dec_input, temp[keys[i]][1], encoder_outputs )\n",
    "                topv, topi = decoder_output.data.topk(kmax)\n",
    "                topv = topv[0].numpy()\n",
    "                topi = topi[0].numpy()\n",
    "                update_dictionary(target_sequence, topv, topi, keys[i], dec_hidden)\n",
    "    \n",
    "    \n",
    "    # Sort the target_Sequence dictionary to keep top 15 sequences only\n",
    "    target_sequence = dict(sorted(target_sequence.items(), key=lambda x: x[1][0], reverse=True)[:kmax])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get the sequence with maximum probability\n",
    "    seq = sorted(target_sequence.items(), key=lambda x: x[1][0], reverse=True)[:1][0][0]\n",
    "    \n",
    "    # Get the decoded words:\n",
    "    decoded_words_indices = seq.split(\"-\")\n",
    "    decoded_words = [output_lang.index2word(int(i)) for i in decoded_words_indices]\n",
    "    \n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni == EOS_token:\n",
    "        decoded_words.append('<EOS>')\n",
    "        break\n",
    "    else:\n",
    "        decoded_words.append(output_lang.index2word[ni])\n",
    "\n",
    "    # Next input is chosen word\n",
    "    decoder_input = Variable(torch.LongTensor([ni]))\n",
    "    if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "# Set back to training mode\n",
    "encoder.train(True)\n",
    "decoder.train(True)\n",
    "\n",
    "return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1-2': [1, 2],\n",
       " '3-4': [3, 20],\n",
       " '4-5': [5, 21],\n",
       " '5-6': [7, 29],\n",
       " '6-7': [9, 22],\n",
       " '7-8-9-19-20': [11, 25]}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = dict()\n",
    "tt.update({\"1-2\":[1,2]})\n",
    "tt.update({\"3-4\":[3,20]})\n",
    "tt.update({\"4-5\":[5,21]})\n",
    "tt.update({\"5-6\":[7,29]})\n",
    "tt.update({\"6-7\":[9,22]})\n",
    "tt.update({\"7-8-9-19-20\":[11,25]})\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7-8-9-19-20'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = sorted(tt.items(), key=lambda x: x[1][0], reverse=True)[:1][0][0]\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8, 9, 19, 20]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = seq.split(\"-\")\n",
    "dec = [output_lang.index2word(int(i)) for i in dec]\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
