{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from masked_cross_entropy import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import io\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import visdom\n",
    "vis = visdom.Visdom()\n",
    "%matplotlib inline\n",
    "\n",
    "USE_CUDA = False\n",
    "\n",
    "MIN_LENGTH = 5\n",
    "MAX_LENGTH = 32\n",
    "\n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        length = Variable(torch.LongTensor(length)).cuda()\n",
    "    else:\n",
    "        length = Variable(torch.LongTensor(length))\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = functional.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3 # Count default tokens\n",
    "\n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed: return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        keep_words = []\n",
    "        \n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words %s / %s = %.4f' % (\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.index_word(word)\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z,.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def read_langs(lang1, lang2, term=\"txt\", reverse=False, normalize=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "\n",
    "    # Attach the path here for the source and target language dataset\n",
    "    #filename = '%s-%s.%s' % (lang1, lang2, term)\n",
    "    \n",
    "    # Short data:\n",
    "    filename = \"en-fr_short.bpe2bpe\";\n",
    "    \n",
    "    # This creats the file directory name whichis used below\n",
    "\n",
    "    # lines contains the data in form of a list \n",
    "    lines = open(filename, encoding=\"utf8\").read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    if normalize == True:\n",
    "        pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    else: \n",
    "        pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def filter_pairs(pairs, MIN_LENGTH, MAX_LENGTH):\n",
    "    filtered_pairs = []\n",
    "    for pair in pairs:\n",
    "        if len(pair[0]) >= MIN_LENGTH and len(pair[0]) <= MAX_LENGTH \\\n",
    "            and len(pair[1]) >= MIN_LENGTH and len(pair[1]) <= MAX_LENGTH:\n",
    "                filtered_pairs.append(pair)\n",
    "    return filtered_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "\n",
    "    # Get the source and target language class objects and the pairs (x_t, y_t)\n",
    "    \n",
    "    ## 1. normalize in argument ????\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, \"bpe2bpe\",reverse, False)\n",
    "    print(\"Read %d sentence pairs\" % len(pairs))\n",
    "    \n",
    "    ## 2. MIN LENGTH & MAX LENGTH ????\n",
    "    pairs = filter_pairs(pairs, MIN_LENGTH, MAX_LENGTH)\n",
    "    print(\"Filtered to %d pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "    \n",
    "    print('Indexed %d words in input language, %d words in output' % (input_lang.n_words, output_lang.n_words))\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "# Return a list of indexes, one for each word in the sentence, plus EOS\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "# Pad a with the PAD symbol\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq\n",
    "\n",
    "\n",
    "def random_batch(batch_size):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "\n",
    "    # Choose random pairs\n",
    "    for i in range(batch_size):\n",
    "        pair = random.choice(pairs)\n",
    "        input_seqs.append(indexes_from_sentence(input_lang, pair[0]))\n",
    "        target_seqs.append(indexes_from_sentence(output_lang, pair[1]))\n",
    "\n",
    "    # Zip into pairs, sort by length (descending), unzip\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    \n",
    "    # For input and target sequences, get array of lengths and pad with 0s to max length\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "    # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n",
    "    input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "    target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_var = input_var.cuda()\n",
    "        target_var = target_var.cuda()\n",
    "        \n",
    "    return input_var, input_lengths, target_var, target_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 25000 sentence pairs\n",
      "Filtered to 1182 pairs\n",
      "Indexing words...\n",
      "Indexed 1342 words in input language, 1506 words in output\n",
      "keep_words 152 / 1339 = 0.1135\n",
      "keep_words 155 / 1503 = 0.1031\n",
      "Trimmed from 1182 pairs to 227, 0.1920 of total\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data('en', 'fr', False)\n",
    "\n",
    "# TRIMMING DATA:\n",
    "# Trimming is optional but could be done to reduce the data size and make processing faster\n",
    "# Removes words with frequency < 5\n",
    "\n",
    "MIN_COUNT = 5\n",
    "\n",
    "input_lang.trim(MIN_COUNT)\n",
    "output_lang.trim(MIN_COUNT)\n",
    "\n",
    "\n",
    "keep_pairs = []\n",
    "\n",
    "for pair in pairs:\n",
    "    input_sentence = pair[0]\n",
    "    output_sentence = pair[1]\n",
    "    keep_input = True\n",
    "    keep_output = True\n",
    "\n",
    "    for word in input_sentence.split(' '):\n",
    "        if word not in input_lang.word2index:\n",
    "            keep_input = False\n",
    "            break\n",
    "\n",
    "    for word in output_sentence.split(' '):\n",
    "        if word not in output_lang.word2index:\n",
    "            keep_output = False\n",
    "            break\n",
    "\n",
    "    # Remove if pair doesn't match input and output conditions\n",
    "    if keep_input and keep_output:\n",
    "        keep_pairs.append(pair)\n",
    "\n",
    "print(\"Trimmed from %d pairs to %d, %.4f of total\" % (len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "pairs = keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size #no of words in the input Language\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None): # hidden vector starts with zero (a guess!)\n",
    "        \n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        embedded = self.embedding(input_seqs) # size = (max_length, batch_size, embed_size). NOTE: embed_size = hidden size here\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) # size = (max_length * batch_size, embed_size)\n",
    "        \n",
    "        outputs, hidden = self.gru(packed, hidden) # outputs are supposed to be probability distribution right?\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        return outputs, hidden\n",
    "\n",
    "    #output (seq_len, batch, hidden_size * num_directions): tensor containing the output features h_t from the last layer of the \n",
    "    # RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.\n",
    "    \n",
    "    # h_n (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t=seq_len\n",
    "    \n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "\n",
    "        if USE_CUDA:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                #attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "                attn_energies[b, i] = self.score(hidden[b,:], encoder_outputs[i, b])\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_energies).unsqueeze(1)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            #print(torch.cat((hidden, encoder_output), 1), attn)\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 0))\n",
    "            #energy = self.v.dot(energy)\n",
    "            #print(self.v.squeeze(0), energy)\n",
    "            energy = (self.v.squeeze(0)).dot(energy)\n",
    "            return energy\n",
    "        \n",
    "##############################################################################################\n",
    "#########################  BAHDANAU_ATTN_DECODER_RNN  ########################################\n",
    "##############################################################################################\n",
    "        \n",
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        ## 3. self.max_length = max_length\n",
    "        self.max_length = MAX_LENGTH\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn('concat', hidden_size)\n",
    "        \n",
    "        # Modifications made below in 2 lines\n",
    "        self.gru = nn.GRU(2*hidden_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size) # use of linear layer ?\n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        # TODO: FIX BATCHING\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, word_input.data.shape[0], -1) # S=1 x B x N , ## N = hidden size (doubt)\n",
    "        #print(word_embedded.size())\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        #print(attn_weights.size())\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        context = context.transpose(0, 1) # 1 x B x N\n",
    "        #print(context.size(), word_embedded.size(), self.output_size)\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2) # 1 x B x 2N (There seems to be a mistake here)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        \n",
    "        ## Modification made below too\n",
    "        #output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context.squeeze(0)), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        \n",
    "        # print(self.output.size(), output.size(), hidden.size(), attn_weights.size())\n",
    "        # 330, torch.Size([80, 330]) torch.Size([2, 80, 1024]) torch.Size([80, 1, 10])\n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr = '1-2-3'\n",
    "rr.split(\"-\")[-1] if len(rr) > 1 else rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEAM SEARCH:\n",
    "    # A dictinary is maintained which keeps track of the sequence within top first k probabilities\n",
    "    # If k = 1, it is same as greedy search. \n",
    "    # The dictinary is of the form {sequence: [probability, decoder_hidden, decoder_attention] }\n",
    "    # The dictinary data type is  {string: [int, torch.Size([num_directions, batch_size, hidden_size]), torch.Size([max_length+1, max_length+1])]}\n",
    "\n",
    "\n",
    "def update_dictionary(target_sequence, topv, topi, key, dec_hidden, decoder_attns):\n",
    "    if len(target_sequence) == 0:\n",
    "        for i in range(len(topi)):\n",
    "            target_sequence.update({str(topi[i]) : [topv[i], dec_hidden, decoder_attns] })\n",
    "    else:\n",
    "        prev_val = target_sequence[key][0]\n",
    "        for i in range(len(topi)):\n",
    "            target_sequence.update({key+\"-\"+str(topi[i]) : [topv[i]*prev_val, dec_hidden, decoder_attns] })\n",
    "        del[target_sequence[key]]\n",
    "        \n",
    "\n",
    "def get_seq_through_beam_search(max_length, decoder, decoder_input, decoder_hidden, decoder_attentions, encoder_outputs, kmax ):\n",
    "    \n",
    "    target_sequence = dict()\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        \n",
    "        if di == 0:\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs )\n",
    "            topv, topi = decoder_output.data.topk(kmax)\n",
    "            topv = topv[0].numpy()\n",
    "            topi = topi[0].numpy()\n",
    "            decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "            update_dictionary(target_sequence, topv, topi, None, decoder_hidden, decoder_attentions)\n",
    "        else:\n",
    "            temp = target_sequence.copy()\n",
    "            keys = list(temp.keys())\n",
    "            for i in range(len(keys)):\n",
    "                inp = int(keys[i].split(\"-\")[-1] if len(keys[i]) > 1 else keys[i])\n",
    "                if inp != EOS_token:\n",
    "                    dec_input = Variable(torch.LongTensor([inp]))\n",
    "                    decoder_output, dec_hidden, decoder_attention = decoder( dec_input, temp[keys[i]][1], encoder_outputs )\n",
    "                    topv, topi = decoder_output.data.topk(kmax)\n",
    "                    topv = topv[0].numpy()\n",
    "                    topi = topi[0].numpy()\n",
    "                    dec_attns = temp[keys[i]][2]\n",
    "                    dec_attns[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "                    update_dictionary(target_sequence, topv, topi, keys[i], dec_hidden, dec_attns)\n",
    "        \n",
    "        # Sort the target_Sequence dictionary and keep top k sequences only\n",
    "        target_sequence = dict(sorted(target_sequence.items(), key=lambda x: x[1][0], reverse=True)[:kmax])\n",
    "     \n",
    "    # Get the sequence, decoder_attentions with maximum probability\n",
    "    pair = sorted(target_sequence.items(), key=lambda x: x[1][0], reverse=True)[:1][0]\n",
    "    seq = pair[0]\n",
    "    decoder_attentions = pair[1][2]\n",
    "    \n",
    "    # Get the decoded words:\n",
    "    decoded_words_indices = seq.split(\"-\")\n",
    "    decoded_words = [output_lang.index2word[int(i)] for i in decoded_words_indices]\n",
    "    if int(decoded_words_indices[-1]) != EOS_token:\n",
    "        decoded_words.append('<EOS>')\n",
    "    \n",
    "    return decoded_words, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))\n",
    "\n",
    "# Evaluation is mostly the same as training, but there are no targets. Instead we always feed the decoder's predictions back to itself. \n",
    "# Every time it predicts a word, we add it to the output string. If it predicts the EOS token we stop there. We also store the decoder's attention outputs for each step to display later.\n",
    "\n",
    "def evaluate(input_seq, max_length=MAX_LENGTH):\n",
    "    input_lengths = [len(input_seq)]\n",
    "    input_seqs = [indexes_from_sentence(input_lang, input_seq)]\n",
    "    input_batches = Variable(torch.LongTensor(input_seqs), volatile=True).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_batches = input_batches.cuda()\n",
    "        \n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]), volatile=True) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    # size for decoder_hidden = (B, H)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "    # print(decoder_input.size(), decoder_hidden.size(), encoder_outputs.size())\n",
    "    kmax = 10\n",
    "    decoded_words, decoder_attentions = get_seq_through_beam_search(max_length, decoder, decoder_input, decoder_hidden, decoder_attentions, encoder_outputs, kmax )\n",
    "\n",
    "    #decoded_words, decoder_attentions = get_seq_through_beam_search(len(input_seq), decoder, decoder_input, decoder_hidden, decoder_attentions, encoder_outputs, kmax )\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:len(decoded_words)+1, :len(encoder_outputs)]\n",
    "\n",
    "\n",
    "# We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:\n",
    "def evaluate_randomly():\n",
    "    [input_sentence, target_sentence] = random.choice(pairs)\n",
    "    evaluate_and_show_attention(input_sentence, target_sentence)\n",
    "\n",
    "def show_plot_visdom():\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf)\n",
    "    buf.seek(0)\n",
    "    attn_win = 'attention (%s)' % hostname\n",
    "    vis.image(torchvision.transforms.ToTensor()(Image.open(buf)), win=attn_win, opts={'title': attn_win})\n",
    "\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_and_show_attention(input_sentence, target_sentence=None):\n",
    "    output_words, attentions = evaluate(input_sentence)\n",
    "    \n",
    "    # Calculating the bleu score excluding the last word (<EOS>)\n",
    "    #bleu_score = nltk.translate.bleu_score.sentence_bleu([target_sentence], ' '.join(output_words[:-1]))\n",
    "    \n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', input_sentence)\n",
    "    if target_sentence is not None:\n",
    "        print('=', target_sentence)\n",
    "    print('<', output_sentence)\n",
    "    #print(\"BLUE SCORE IS:\", bleu_score)\n",
    "    \n",
    "    show_attention(input_sentence, output_words, attentions)\n",
    "    \n",
    "    # Show input, target, output text in visdom\n",
    "    win = 'evaluted (%s)' % hostname\n",
    "    text = '<p>&gt; %s</p><p>= %s</p><p>&lt; %s</p>' % (input_sentence, target_sentence, output_sentence)\n",
    "    vis.text(text, win=win, opts={'title': win})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(input_batches, input_lengths, target_batches, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, max_length=MAX_LENGTH):\n",
    "    \n",
    "    # Added 2 lines below\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "\n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size))\n",
    "    \n",
    "    # Q.) Why do we use the last hidden state for t = 0. is of because z_t = phi_init(context_vector)\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size))\n",
    "\n",
    "    # Move new Variables to CUDA\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "    # Loss calculation and backpropagation\n",
    "    loss = masked_cross_entropy(\n",
    "        all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_lengths\n",
    "    )\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0], ec, dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 5a1cb649bb92582e4f253cea at 2017-11-27 20:05:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:39: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.3. Note that arange generates values in [start; end), not [start; end].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] 2m 17s (2) 3.9746\n",
      "2m 17s (- 14m 55s) (2 13%) 4.5133\n",
      "> Thank you very much .\n",
      "= Merci beaucoup .\n",
      "< Et , EOS\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "cannot write mode RGBA as JPEG",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\akash\\Anaconda3\\lib\\site-packages\\PIL\\JpegImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0mrawmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRAWMODE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'RGBA'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a6ba6177ede6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mevaluate_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mevaluate_randomly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mplot_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-78f8454a94b2>\u001b[0m in \u001b[0;36mevaluate_randomly\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_randomly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_sentence\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mevaluate_and_show_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mshow_plot_visdom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-78f8454a94b2>\u001b[0m in \u001b[0;36mevaluate_and_show_attention\u001b[0;34m(input_sentence, target_sentence)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[1;31m#print(\"BLUE SCORE IS:\", bleu_score)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mshow_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattentions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[1;31m# Show input, target, output text in visdom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-78f8454a94b2>\u001b[0m in \u001b[0;36mshow_attention\u001b[0;34m(input_sentence, output_words, attentions)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_major_locator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultipleLocator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mshow_plot_visdom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-78f8454a94b2>\u001b[0m in \u001b[0;36mshow_plot_visdom\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mattn_win\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'attention (%s)'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mhostname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattn_win\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mattn_win\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mshow_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattentions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\akash\\Anaconda3\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\akash\\Anaconda3\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36mimage\u001b[0;34m(self, img, win, env, opts)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'JPEG'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'jpgquality'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0mb64encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb64\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb64encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\akash\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   1926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1929\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m             \u001b[1;31m# do what we can to clean up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\akash\\Anaconda3\\lib\\site-packages\\PIL\\JpegImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRAWMODE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot write mode %s as JPEG\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoderinfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: cannot write mode RGBA as JPEG"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAewAAAFrCAYAAAATn7oAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntiRJQ+D/A0qT7iN+x0soAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1841b726048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configure models\n",
    "# attn_model = 'dot'\n",
    "hidden_size = 1024\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 80\n",
    "# batch_size = 50\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "#clip = 1.0 # Based on our paper, clipping gradient norm is 1\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "#n_epochs = 50000\n",
    "n_epochs = 15\n",
    "epoch = 0\n",
    "#plot_every = 20\n",
    "#print_every = 100\n",
    "#evaluate_every = 10000 # We check the validation in every 10,000 minibatches\n",
    "\n",
    "plot_every = 2\n",
    "print_every = 2\n",
    "evaluate_every = 2 # We check the validation in every 10,000 minibatches\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers, dropout=dropout)\n",
    "decoder = BahdanauAttnDecoderRNN( hidden_size, output_lang.n_words, n_layers, dropout_p=dropout)\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "import sconce\n",
    "job = sconce.Job('seq2seq-translate', {\n",
    "    #'attn_model': attn_model,\n",
    "    'n_layers': n_layers,\n",
    "    'dropout': dropout,\n",
    "    'hidden_size': hidden_size,\n",
    "    'learning_rate': learning_rate,\n",
    "    'clip': clip,\n",
    "    'teacher_forcing_ratio': teacher_forcing_ratio,\n",
    "    'decoder_learning_ratio': decoder_learning_ratio,\n",
    "})\n",
    "job.plot_every = plot_every\n",
    "job.log_every = print_every\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "######                         PART-III : Modeling                   #####\n",
    "##########################################################################\n",
    "\n",
    "ecs = []\n",
    "dcs = []\n",
    "eca = 0\n",
    "dca = 0\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    epoch += 1\n",
    "\n",
    "    # Get training data for this cycle\n",
    "    input_batches, input_lengths, target_batches, target_lengths = random_batch(batch_size)\n",
    "    #print(input_batches.size())\n",
    "    \n",
    "    # Run the train function\n",
    "    loss, ec, dc = train(\n",
    "        input_batches, input_lengths, target_batches, target_lengths,\n",
    "        encoder, decoder,\n",
    "        encoder_optimizer, decoder_optimizer)\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "    eca += ec\n",
    "    dca += dc\n",
    "\n",
    "    job.record(epoch, loss)\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "\n",
    "    if epoch % evaluate_every == 0:\n",
    "        evaluate_randomly()\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "\n",
    "        # TODO: Running average helper\n",
    "        ecs.append(eca / plot_every)\n",
    "        dcs.append(dca / plot_every)\n",
    "        ecs_win = 'encoder grad (%s)' % hostname\n",
    "        dcs_win = 'decoder grad (%s)' % hostname\n",
    "        vis.line(np.array(ecs), win=ecs_win, opts={'title': ecs_win})\n",
    "        vis.line(np.array(dcs), win=dcs_win, opts={'title': dcs_win})\n",
    "        eca = 0\n",
    "        dca = 0"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
