{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "#from comet_ml import Experiment\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import io\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import subprocess \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Code Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# candidates: \n",
    "# bpe2char_3_121217__0.000721_1024_8_Bahdanau\n",
    "# bpe2char_3_121417__0.0008_256_5_Luong\n",
    "# bpe2char_3_121317__0.000721_1024_10_Bahdanau\n",
    "# bpe2char_3_121317__0.000941_1024_15_Bahdanau\n",
    "# bpe2char_3_121117__0.000321_512_8_Bahdanau # bleu scores around 2\n",
    "# bpe2char_3_121317__0.000121_1024_15_Bahdanau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_to_test = \"bpe2char_3_121117__0.000321_512_8_Bahdanau\"\n",
    "#epoch_eval = None\n",
    "epoch_eval = 4200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#out_dir = \"/scratch/eff254/NLP/MICA_NeuralMachineTranslation/EduTrials/FinalModels/checkpoints\"\n",
    "#out_dir = \"/scratch/rds491/MICA_NeuralMachineTranslation/EduTrials/FinalModels/checkpoints\" \n",
    "out_dir = \"/scratch/mmd378/NLP_2017/edu_models_120917/checkpoints\"\n",
    "# If running things from Raul, getfacl on his checkpoints AND Evaluation folders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Epoch for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_number(filename):\n",
    "    \n",
    "    number = re.split(\"_\", filename, maxsplit=0, flags=0)[-1]\n",
    "    number = re.split(\"[.]\", number, maxsplit=0, flags=0)[0]\n",
    "    \n",
    "    try: \n",
    "        int(number)\n",
    "        return int(number)\n",
    "    except ValueError:\n",
    "        return 0 # A filter for opt files\n",
    "    \n",
    "def get_max_iteration(): \n",
    "\n",
    "    files = os.listdir(\"{}/{}/\".format(out_dir, model_to_test))\n",
    "    iterations = [get_number(x) for x in files]\n",
    "    \n",
    "    return np.max(iterations)\n",
    "\n",
    "if epoch_eval is not None: \n",
    "    epoch = epoch_eval \n",
    "else: \n",
    "    epoch = get_max_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "opt = pickle.load(open(\"{}/{}/model_opt.p\".format(out_dir, model_to_test), \"rb\"))\n",
    "opt.MAX_LENGTH=200\n",
    "\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_token = 3\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}\n",
    "        self.n_words = 4 # Count default tokens\n",
    "\n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed: return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words %s / %s = %.4f' % (\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}\n",
    "        self.n_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.index_word(word)\n",
    "\n",
    "def read_langs(lang1, lang2, set_type=\"train\", term=\"txt\", reverse=False, normalize=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    if set_type == \"train\":\n",
    "        filename = '%s/train/%s-%s.%s' % (opt.main_data_dir, lang1, lang2, term)\n",
    "    elif set_type == \"dev\":\n",
    "        filename = '%s/dev/%s-%s.%s' % (opt.main_data_dir, lang1, lang2, term)\n",
    "    elif set_type == \"valid\":\n",
    "        filename = '%s/dev/%s-%s.%s' % (opt.main_data_dir, lang1, lang2, term)\n",
    "    elif set_type == \"tst2010\":\n",
    "        filename = '%s/test/%s-%s.tst2010-%s' % (opt.main_data_dir, lang1, lang2, term)\n",
    "    elif set_type == \"tst2011\":\n",
    "        filename = '%s/test/%s-%s.tst2011-%s' % (opt.main_data_dir, lang1, lang2, term)\n",
    "    elif set_type == \"tst2012\":\n",
    "        filename = '%s/test/%s-%s.tst2012-%s' % (opt.main_data_dir, lang1, lang2, term)\n",
    "    elif set_type == \"tst2013\":\n",
    "        filename = '%s/test/%s-%s.tst2013-%s' % (opt.main_data_dir, lang1, lang2, term)\n",
    "    elif set_type == \"tst2014\":\n",
    "        filename = '%s/test/%s-%s.tst2014-%s' % (opt.main_data_dir, lang1, lang2, term)\n",
    "    else:\n",
    "        raise ValueError(\"set_type not found. Check data folder options\")\n",
    "\n",
    "\n",
    "    # lines contains the data in form of a list\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    if normalize == True:\n",
    "        pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    else:\n",
    "        pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_lang = pickle.load(open(\"{}/{}/input_lang.p\".format(out_dir, model_to_test), \"rb\"))\n",
    "output_lang = pickle.load(open(\"{}/{}/output_lang.p\".format(out_dir, model_to_test), \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Main model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading/Updating encoder state dict at epoch 4200\n",
      "Loading/Updating decoder state dict at epoch 4200\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# 3. Main model encoder - decoder #\n",
    "###################################\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size #no of words in the input Language\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "\n",
    "\n",
    "    def forward(self, input_seqs, input_lengths, hidden=None): # hidden vector starts with zero (a guess!)\n",
    "\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        embedded = self.embedding(input_seqs) # size = (max_length, batch_size, embed_size). NOTE: embed_size = hidden size here\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) # size = (max_length * batch_size, embed_size)\n",
    "\n",
    "        outputs, hidden = self.gru(packed, hidden) # outputs are supposed to be probability distribution right?\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        return outputs, hidden\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "\n",
    "        if opt.USE_CUDA:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(hidden[b,:], encoder_outputs[i, b])\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_energies).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_output):\n",
    "\n",
    "        if self.method == 'dot':\n",
    "            # Used by Luong\n",
    "            energy = hidden.squeeze(0).dot(encoder_output)\n",
    "            return energy\n",
    "\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            # Used by Bahdanau\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 0))\n",
    "            energy = (self.v.squeeze(0)).dot(energy)\n",
    "            return energy\n",
    "\n",
    "###############################\n",
    "#  BAHDANAU_ATTN_DECODER_RNN  #\n",
    "###############################\n",
    "\n",
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        ## 3. self.max_length = max_length\n",
    "        ## self.max_length = opt.MAX_LENGTH\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn('concat', hidden_size)\n",
    "\n",
    "        # Modifications made below in 2 lines\n",
    "        self.gru = nn.GRU(2*hidden_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        # self.out = nn.Linear(hidden_size * 2, output_size) # use of linear layer ?\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, word_input.data.shape[0], -1) # S=1 x B x N , ## N = hidden size (doubt)\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        context = context.transpose(0, 1) # 1 x B x N\n",
    "\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2) # 1 x B x 2N (There seems to be a mistake here)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        output = F.log_softmax(self.out(output))\n",
    "        # output = F.log_softmax(self.out(torch.cat((output, context.squeeze(0)), 1)))\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "############################\n",
    "#  LUONG_ATTN_DECODER_RNN  #\n",
    "############################\n",
    "\n",
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "            # Note: we run this one step at a time\n",
    "\n",
    "            # Get the embedding of the current input word (last output word)\n",
    "            batch_size = input_seq.size(0)\n",
    "            embedded = self.embedding(input_seq)\n",
    "            embedded = self.embedding_dropout(embedded)\n",
    "            embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "\n",
    "            # Get current hidden state from input word and last hidden state\n",
    "            rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "            # Calculate attention from current RNN state and all encoder outputs;\n",
    "            # apply to encoder outputs to get weighted average \n",
    "\n",
    "            attn_weights = self.attn(rnn_output.transpose(0, 1), encoder_outputs) # B*1*S encoder_outputs: S*B*emb\n",
    "            context = attn_weights.bmm(encoder_outputs.transpose(0, 1)).squeeze(1)\n",
    "            # Attentional vector using the RNN hidden state and context vector\n",
    "            # concatenated together (Luong eq. 5)        \n",
    "            rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "            # context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "            concat_input = torch.cat((rnn_output, context), 1)\n",
    "            concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "            # Finally predict next token (Luong eq. 6, without softmax & logsigmoid)\n",
    "            output = F.logsigmoid(self.out(concat_output))\n",
    "\n",
    "            # Return final output, hidden state, and attention weights (for visualization)\n",
    "            return output, hidden, attn_weights\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, opt.hidden_size, opt.n_layers, dropout=opt.dropout)\n",
    "\n",
    "if opt.attention == 'Luong':\n",
    "    decoder = LuongAttnDecoderRNN('dot', opt.hidden_size, output_lang.n_words, opt.n_layers, dropout=opt.dropout)\n",
    "elif opt.attention == 'Bahdanau':\n",
    "    decoder = BahdanauAttnDecoderRNN( opt.hidden_size, output_lang.n_words, opt.n_layers, dropout_p=opt.dropout)\n",
    "else: \n",
    "    raise ValueError('Attention not found: Options are Luong or Bahdanau')   \n",
    "    \n",
    "if opt.USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "print(\"Loading/Updating encoder state dict at epoch {}\".format(epoch))\n",
    "enc_state = torch.load(\"{}/{}/saved_encoder_{}.pth\".format(out_dir, model_to_test, epoch))\n",
    "encoder.load_state_dict(enc_state)\n",
    "\n",
    "print(\"Loading/Updating decoder state dict at epoch {}\".format(epoch))\n",
    "dec_state = torch.load(\"{}/{}/saved_decoder_{}.pth\".format(out_dir, model_to_test, epoch))\n",
    "decoder.load_state_dict(dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Valuation framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def indexes_from_sentence(lang, sentence):\n",
    "    try:\n",
    "        val = [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    except KeyError:\n",
    "        # Do it individually. Means one word is not on dictionary:\n",
    "        val = []\n",
    "        for word in sentence.split(' '):\n",
    "            try:\n",
    "                indexed = lang.word2index[word]\n",
    "                val.append(indexed)\n",
    "            except KeyError:\n",
    "                val.append(3)\n",
    "\n",
    "    return val + [EOS_token]\n",
    "\n",
    "def update_dictionary(target_sequence, topv, topi, key, dec_hidden, decoder_attns):\n",
    "    if len(target_sequence) == 0:\n",
    "        for i in range(len(topi)):\n",
    "            target_sequence.update({str(topi[i]) : [topv[i], dec_hidden, decoder_attns] })\n",
    "    else:\n",
    "        prev_val = target_sequence[key][0]\n",
    "        for i in range(len(topi)):\n",
    "            target_sequence.update({key+\"-\"+str(topi[i]) : [topv[i]+prev_val, dec_hidden, decoder_attns] })\n",
    "        del[target_sequence[key]]\n",
    "\n",
    "\n",
    "def get_seq_through_beam_search(max_length, decoder, decoder_input, decoder_hidden, decoder_attentions, encoder_outputs, kmax ):\n",
    "\n",
    "    target_sequence = dict()\n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "\n",
    "        if di == 0:\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs )\n",
    "            topv, topi = decoder_output.data.topk(kmax)\n",
    "            topv = topv[0].cpu().numpy()\n",
    "            topi = topi[0].cpu().numpy()\n",
    "            decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "            update_dictionary(target_sequence, topv, topi, None, decoder_hidden, decoder_attentions)\n",
    "        else:\n",
    "            temp = target_sequence.copy()\n",
    "            keys = list(temp.keys())\n",
    "            for i in range(len(keys)):\n",
    "                inp = int(keys[i].split(\"-\")[-1] if len(keys[i]) > 1 else keys[i])\n",
    "                if inp != EOS_token:\n",
    "                    dec_input = Variable(torch.LongTensor([inp]))\n",
    "                    dec_input = dec_input.cuda() if opt.USE_CUDA else dec_input\n",
    "                    decoder_output, dec_hidden, decoder_attention = decoder( dec_input, temp[keys[i]][1], encoder_outputs )\n",
    "                    topv, topi = decoder_output.data.topk(kmax)\n",
    "                    topv = topv[0].cpu().numpy()\n",
    "                    topi = topi[0].cpu().numpy()\n",
    "                    dec_attns = temp[keys[i]][2]\n",
    "                    dec_attns[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "                    update_dictionary(target_sequence, topv, topi, keys[i], dec_hidden, dec_attns)\n",
    "                    if topv.any() == EOS_token:\n",
    "                        break\n",
    "\n",
    "        # Sort the target_Sequence dictionary and keep top k sequences only\n",
    "        target_sequence = dict(sorted(target_sequence.items(), key=lambda x: x[1][0], reverse=True)[:kmax])\n",
    "\n",
    "    # Get the sequence, decoder_attentions with maximum probability\n",
    "    pair = sorted(target_sequence.items(), key=lambda x: x[1][0], reverse=True)[:1][0]\n",
    "    seq = pair[0]\n",
    "    decoder_attentions = pair[1][2]\n",
    "\n",
    "    # Get the decoded words:\n",
    "    decoded_words_indices = seq.split(\"-\")\n",
    "    decoded_words = [output_lang.index2word[int(i)] for i in decoded_words_indices]\n",
    "    if int(decoded_words_indices[-1]) != EOS_token:\n",
    "        decoded_words.append('<EOS>')\n",
    "\n",
    "    return decoded_words, decoder_attentions\n",
    "\n",
    "# Evaluation is mostly the same as training, but there are no targets. Instead we always feed the decoder's predictions back to itself.\n",
    "# Every time it predicts a word, we add it to the output string. If it predicts the EOS token we stop there. We also store the decoder's attention outputs for each step to display later.\n",
    "\n",
    "def evaluate(input_seq):\n",
    "\n",
    "    max_length = opt.MAX_LENGTH #len(input_seq.split(' '))\n",
    "\n",
    "    #input_lengths = [len(input_seq)]\n",
    "    input_lengths = [max_length]\n",
    "    input_seqs = [indexes_from_sentence(input_lang, input_seq)]\n",
    "    input_batches = Variable(torch.LongTensor(input_seqs), volatile=True).transpose(0, 1)\n",
    "\n",
    "    if opt.USE_CUDA:\n",
    "        input_batches = input_batches.cuda()\n",
    "\n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "\n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]), volatile=True) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "\n",
    "    if opt.USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "    decoded_words, decoder_attentions = get_seq_through_beam_search(max_length, decoder, decoder_input, decoder_hidden, decoder_attentions, encoder_outputs, opt.kmax )\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "\n",
    "    return decoded_words, decoder_attentions[:len(decoded_words)+1, :len(encoder_outputs)]\n",
    "\n",
    "# We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:\n",
    "def evaluate_randomly(pairs2eval):\n",
    "    [input_sentence, target_sentence] = random.choice(pairs2eval)\n",
    "    evaluate_and_show_attention(input_sentence, target_sentence)\n",
    "\n",
    "def evaluate_and_show_attention(input_sentence, target_sentence=None):\n",
    "    output_words, attentions = evaluate(input_sentence)\n",
    "\n",
    "    # Calculating the bleu score excluding the last word (<EOS>)\n",
    "    #bleu_score = nltk.translate.bleu_score.sentence_bleu([target_sentence], ' '.join(output_words[:-1]))\n",
    "\n",
    "    output_sentence = ' '.join(output_words)\n",
    "\n",
    "    print('>', input_sentence)\n",
    "    if target_sentence is not None:\n",
    "        print('=', target_sentence)\n",
    "    print('<', output_sentence)\n",
    "    #print(\"BLUE SCORE IS:\", bleu_score)   \n",
    "\n",
    "def undo_chars(string): \n",
    "    \n",
    "    #string = re.sub(\"   \", \"@\", string)\n",
    "    #string = re.sub(\" \", \"\", string)\n",
    "    #string = re.sub(\"@\", \" \", string)\n",
    "    strings = string.split(\"   \")\n",
    "    strings = [''.join(x.split(' ')) for x in strings]\n",
    "    string = \" \".join(strings)\n",
    "    return string\n",
    "\n",
    "def undo_bpe(string): \n",
    "    \n",
    "    string = re.sub(\"@@ \", \"\", string)\n",
    "        \n",
    "    return string\n",
    "    \n",
    "def eval_single(string):\n",
    "    \n",
    "    words, tensor = evaluate(string)\n",
    "    words = ' '.join(words)\n",
    "    words = re.sub('<EOS>', '', words)\n",
    "    words = re.sub('EOS', '', words)\n",
    "    \n",
    "\n",
    "    return(words)    \n",
    "    \n",
    "\n",
    "def evaluate_list_pairs(list_strings, term=opt.model_type):\n",
    "    \n",
    "    if term == \"bpe2bpe\":\n",
    "        output = [undo_bpe(eval_single(x[0])) for x in list_strings]\n",
    "    elif term in [\"bpe2char\", \"bpe2char_2\", \"bpe2char_3\"]:\n",
    "        output = [undo_chars(eval_single(x[0])) for x in list_strings]\n",
    "    else:\n",
    "        output = [eval_single(x[0]) for x in list_strings]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def export_as_list(original, translations): \n",
    "    \n",
    "    with open(\"{}/{}/original_tst_md2.txt\".format(opt.eval_dir, opt.experiment), 'w') as original_file:\n",
    "        for sentence in original:\n",
    "            original_file.write(sentence + \"\\n\")\n",
    "    \n",
    "    \n",
    "    with open(\"{}/{}/translations_tst_md2.txt\".format(opt.eval_dir, opt.experiment), 'w') as translations_file:\n",
    "        for sentence in translations:\n",
    "            translations_file.write(sentence + \"\\n\")\n",
    "        \n",
    "def run_perl(): \n",
    "    \n",
    "    ''' Assumes the multi-bleu.perl is in opt.eval_dir\n",
    "        Assumes you exported files with names in export_as_list()'''\n",
    "    \n",
    "    cmd = \"%s %s < %s\" % (opt.eval_dir + \"./multi-bleu.perl\", opt.eval_dir + opt.experiment + \\\n",
    "        '/original_tst_md2.txt', opt.eval_dir + opt.experiment + '/translations_tst_md2.txt')\n",
    "    print(\"Estimating BLEU score...\")\n",
    "    bleu_output = subprocess.check_output(cmd, shell=True)\n",
    "    m = re.search(\"BLEU = (.+?),\", str(bleu_output))\n",
    "    bleu_score = float(m.group(1))\n",
    "    \n",
    "    return bleu_score\n",
    "\n",
    "    \n",
    "def multi_blue_dev(dev_pairs, term=opt.model_type):\n",
    "    prediction = evaluate_list_pairs(dev_pairs)\n",
    "    if term == \"bpe2bpe\":\n",
    "        target_eval = [undo_bpe(x[1]) for x in dev_pairs]   \n",
    "    elif term in [\"bpe2char\", \"bpe2char_2\", \"bpe2char_3\"]:\n",
    "        target_eval = [undo_chars(x[1]) for x in dev_pairs]   \n",
    "    else:\n",
    "        target_eval = [x[1] for x in dev_pairs] \n",
    "    \n",
    "    export_as_list(target_eval, prediction)\n",
    "    blue = run_perl()\n",
    "    return blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Run evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Reading lines...\n",
      "Reading lines...\n",
      "Evaluating 2014...\n",
      "Estimating BLEU score...\n",
      "Bleu score 2014 - bpe2char_3_121117__0.000321_512_8_Bahdanau - epoch 4200 = 2.55\n",
      "Evaluating 2013...\n"
     ]
    }
   ],
   "source": [
    "test2012 = read_langs(\"en\", \"fr\", set_type=\"tst2012\", term=\"bpe2char_3\")\n",
    "test2013 = read_langs(\"en\", \"fr\", set_type=\"tst2013\", term=\"bpe2char_3\")\n",
    "test2014 = read_langs(\"en\", \"fr\", set_type=\"tst2014\", term=\"bpe2char_3\")\n",
    "\n",
    "print(\"Evaluating 2014...\")\n",
    "blue_score14 = multi_blue_dev(test2014)\n",
    "print(\"Bleu score 2014 - {} - epoch {} = {}\".format(model_to_test, epoch, blue_score14)) \n",
    "print(\"Evaluating 2013...\")\n",
    "blue_score13 = multi_blue_dev(test2013)\n",
    "print(\"Bleu score 2013 - {} - epoch {} = {}\".format(model_to_test, epoch, blue_score13)) \n",
    "print(\"Evaluating 2012...\")\n",
    "blue_score12 = multi_blue_dev(test2012)\n",
    "print(\"Bleu score 2012 - {} - epoch {} = {}\".format(model_to_test, epoch, blue_score12)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-da17e953c4f2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-da17e953c4f2>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Quand j'avais la vingtaine, j'ai vu mes tout premiers clients comme psychothérapeute.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Quand j'avais la vingtaine, j'ai vu mes tout premiers clients comme psychothérapeute.\n",
    "J'étais étudiante en thèse en psychologie clinique à Berkeley.\n",
    "Elle, c'était une femme de 26 ans appelée Alex.\n",
    "Lorsqu'Alex est entrée pour sa première séance, elle portait un jean et un grand top trop large, elle s'est laissée tomber sur le canapé de mon bureau, a enlevé ses chaussures et m'a dit qu'elle était ici pour parler de problèmes de garçons.\n",
    "Lorsque j'ai entendu ça, j'ai été si soulagée.\n",
    "\n",
    "\n",
    "QJ'était jamais étrer.EOS\n",
    "C'était un trouve à ça.EOS\n",
    "C'était une autre.EOS\n",
    "Qain avait une autre vie EOS\n",
    "C'est juste artiste.EOS\n",
    "~                                \n",
    "\n",
    "\n",
    "\n",
    "Vous savez, c'est la première fois que le problème est le problème de la planète.\n",
    "Par exemple, mais on peut être un peu plus important, mais on peut être pour moi.\n",
    "Alors nous n'avons pas été commencé à la plupart de nombreuses personnes n'avaient pas.\n",
    "Peut-être que vous pouvez être dire que vous ne pourraient pas être différentes.\n",
    "Alors que nous avons beaucoup de nouveau, nous avons beaucoup de nouveau problème.\n",
    "\n",
    "\n",
    "Vous savez que la dissection des cadavres est la manière traditionnelle d'apprendre l'anatomie humaine.\n",
    "Pour les étudiants c'est une sacrée expérience, mais pour une école, cela peut s'avérer très difficile ou coûteux.\n",
    "Et nous savons que la plupart des cours d'anatomie, n'ont pas de laboratoire de dissection de cadavres.\n",
    "C'est peut-être pour ces raisons, où selon où vous êtes, qu'il peut être difficile de se procurer des cadavres.\n",
    "Pour trouver une solution, nous avons développé avec le Dr. Brown à Stanford la table de dissection virtuelle.\n",
    "~                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Reading lines...\n",
      "Reading lines...\n",
      "Evaluating 2013...\n",
      "Estimating BLEU score...\n",
      "Bleu score 2013 - bpe2char_3_121117__0.000321_512_8_Bahdanau - epoch 4400 = 3.28\n",
      "Evaluating 2014...\n"
     ]
    }
   ],
   "source": [
    "test2012 = read_langs(\"en\", \"fr\", set_type=\"tst2012\", term=\"bpe2char_3\")\n",
    "test2013 = read_langs(\"en\", \"fr\", set_type=\"tst2013\", term=\"bpe2char_3\")\n",
    "test2014 = read_langs(\"en\", \"fr\", set_type=\"tst2014\", term=\"bpe2char_3\")\n",
    "\n",
    "# print(\"Evaluating 2012...\")\n",
    "# blue_score12 = multi_blue_dev(test2012[0:20])\n",
    "# print(\"Bleu score 2012 - {} - epoch {} = {}\".format(model_to_test, epoch, blue_score12)) \n",
    "print(\"Evaluating 2013...\")\n",
    "blue_score13 = multi_blue_dev(test2013[0:20])\n",
    "print(\"Bleu score 2013 - {} - epoch {} = {}\".format(model_to_test, epoch, blue_score13)) \n",
    "print(\"Evaluating 2014...\")\n",
    "blue_score14 = multi_blue_dev(test2014[0:20])\n",
    "print(\"Bleu score 2014 - {} - epoch {} = {}\".format(model_to_test, epoch, blue_score14)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
