{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bilingual bpe2char model in pytorch\n",
    "# based on theano code https://github.com/nyu-dl/dl4mt-c2c/blob/master/bpe2char/char_base.py\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cPickle\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from collections import OrderedDict\n",
    "# from mixer import *\n",
    "# mixer contains the functions for model building blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the paper, they mention basing their model on Banhadu et al. \n",
    "# Implementation of this original model in pytorch \n",
    "# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\n",
    "# USE_CUDA = False\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, bidirectional=True)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden\n",
    "\n",
    "# BahdanauAttnDecoderRNN\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn(\"concat\", hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note that we will only be running forward for a single decoder time step, but will use all encoder outputs\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size, max_length=MAX_LENGTH):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(seq_len)) # B x 1 x S\n",
    "        if USE_CUDA: attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
    "        return F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.other.dot(energy)\n",
    "            return energy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base encoder decoder models from pytorch docs \n",
    "\n",
    "\n",
    "\n",
    "# # embedding\n",
    "# params['Wemb'] = norm_weight(options['n_words_src'], options['dim_word_src'])\n",
    "# params['Wemb_dec'] = norm_weight(options['n_words'], options['dim_word'])\n",
    "\n",
    "# # encoder\n",
    "# params = get_layer('gru')[0](options, params,\n",
    "#                              prefix='encoder',\n",
    "#                              nin=options['dim_word_src'],\n",
    "#                              dim=options['enc_dim'])\n",
    "# params = get_layer('gru')[0](options, params,\n",
    "#                              prefix='encoderr',\n",
    "#                              nin=options['dim_word_src'],\n",
    "#                              dim=options['enc_dim'])\n",
    "# ctxdim = 2 * options['enc_dim']\n",
    "\n",
    "# slower and faster layer only used in \"biscale encoder model\" ?\n",
    "# use bidirectional RNN\n",
    "\n",
    "#     # context\n",
    "#     ctx = concatenate([proj, projr[::-1]], axis=proj.ndim-1)\n",
    "\n",
    "#     # context mean\n",
    "#     ctx_mean = (ctx * x_mask[:, :, None]).sum(0) / x_mask.sum(0)[:, None]\n",
    "\n",
    "# def fflayer(tparams, state_below, options, prefix='rconv',\n",
    "#             activ='lambda x: tensor.tanh(x)', **kwargs):\n",
    "#     return eval(activ)(\n",
    "#         tensor.dot(state_below, tparams[_p(prefix, 'W')]) +\n",
    "#         tparams[_p(prefix, 'b')])\n",
    "# def ffflayer(tparams, state_below1, state_below2, options, prefix='rconv',\n",
    "#              activ='lambda x: tensor.tanh(x)', **kwargs):\n",
    "#     return eval(activ)(\n",
    "#         tensor.dot(state_below1, tparams[_p(prefix, 'W')]) +\n",
    "#         tensor.dot(state_below2, tparams[_p(prefix, 'U')]) +\n",
    "#         tparams[_p(prefix, 'b')])\n",
    "\n",
    "\n",
    "class EncoderB2C(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderB2C, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        self.gru1 = nn.GRU(hidden_size, hidden_size) #, bidirectional=True)\n",
    "        self.gru2 = nn.GRU(hidden_size, hidden_size)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.context=None\n",
    "        \n",
    "    def forward(self, in_data, hidden1, hidden2):\n",
    "        embedded = self.embedding(in_data).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        # can potentially accomplish this with bidirectional=True argument\n",
    "        # would need to translate that to the context\n",
    "        output1, hidden1 = self.gru1(output, hidden1)\n",
    "        output2, hidden2 = self.gru2(output, hidden2)\n",
    "        context = torch.concatenate([hidden1, hidden2], axis=hidden1.ndim-1)\n",
    "        context_mean = torch.mean(context)\n",
    "        return output1, output2, hidden1, hidden2, context\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# params['Wemb_dec'] = norm_weight(options['n_words'], options['dim_word'])\n",
    "        \n",
    "# # init_state of decoder\n",
    "# params = get_layer('ff')[0](options, params,\n",
    "#                             prefix='ff_init_state_char',\n",
    "#                             nin=ctxdim,\n",
    "#                             nout=options['dec_dim'])\n",
    "# params = get_layer('ff')[0](options, params,\n",
    "#                             prefix='ff_init_state_word',\n",
    "#                             nin=ctxdim,\n",
    "#                             nout=options['dec_dim'])\n",
    "\n",
    "# print \"target dictionary size: %d\" % options['n_words'] \n",
    "\n",
    "# # decoder\n",
    "# params = get_layer('two_layer_gru_decoder')[0](options, params,\n",
    "#                                                prefix='decoder',\n",
    "#                                                nin=options['dim_word'],\n",
    "#                                                dim_char=options['dec_dim'],\n",
    "#                                                dim_word=options['dec_dim'],\n",
    "#                                                dimctx=ctxdim)\n",
    "  \n",
    "#           'two_layer_gru_decoder': ('param_init_two_layer_gru_decoder',\n",
    "#                                     'two_layer_gru_decoder'),\n",
    " \n",
    "class DecoderB2C(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderB2C, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, in_data, hidden):\n",
    "        output = self.embedding(in_data).view(1, 1, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "## comment\n",
    "# # readout\n",
    "# params = get_layer('fff')[0](options, params, prefix='ff_logit_rnn',\n",
    "#                              nin1=options['dec_dim'], nin2=options['dec_dim'],\n",
    "#                              nout=options['dim_word'], ortho=False)\n",
    "# params = get_layer('ff')[0](options, params, prefix='ff_logit_prev',\n",
    "#                             nin=options['dim_word'],\n",
    "#                             nout=options['dim_word'],\n",
    "#                             ortho=False)\n",
    "# params = get_layer('ff')[0](options, params, prefix='ff_logit_ctx',\n",
    "#                             nin=ctxdim,\n",
    "#                             nout=options['dim_word'],\n",
    "#                             ortho=False)\n",
    "# params = get_layer('ff')[0](options, params, prefix='ff_logit',\n",
    "#                             nin=options['dim_word'],\n",
    "#                             nout=options['n_words'])\n",
    "\n",
    "class SoftAlignment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoftAlignment, self).__init__()\n",
    "        self.C = None\n",
    "        \n",
    "    def forward(self):\n",
    "        return None\n",
    "    def init_weights(self):\n",
    "        return None\n",
    "    \n",
    "# loss function: negative log likelihood \n",
    "nn.NLLLoss()\n",
    "# optimizer: SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_bi_b2c_model():\n",
    "    #     model = \n",
    "    #     optimizer = \n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions defined below (in original code)\n",
    "- def init_params(options):\n",
    " - initializes layers of model for encoder and decoder using \"get_layer\" function from mixer.py\n",
    "\n",
    "- def build_model(tparams, options):\n",
    " - formats data inputs     # description string: #words x #samples\n",
    " - generates embeddings\n",
    " - passes data through GRU\n",
    " \n",
    " \n",
    "- def build_sampler(tparams, options, trng, use_noise):\n",
    " - not sure what samples are for \n",
    "- def gen_sample(tparams, f_init, f_next, x, options, trng=None,k=1, maxlen=500, stochastic=True, argmax=False):\n",
    " - not sure what samples are for\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### ORIGINAL CODE #####\n",
    "'''\n",
    "Build a simple neural language model using GRU units\n",
    "'''\n",
    "# import theano\n",
    "# from theano import tensor\n",
    "# from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "import cPickle\n",
    "import numpy\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from collections import OrderedDict\n",
    "from mixer import *\n",
    "\n",
    "\n",
    "def init_params(options):\n",
    "    params = OrderedDict()\n",
    "\n",
    "    print \"source dictionary size: %d\" % options['n_words_src']\n",
    "    # embedding\n",
    "    params['Wemb'] = norm_weight(options['n_words_src'], options['dim_word_src'])\n",
    "    params['Wemb_dec'] = norm_weight(options['n_words'], options['dim_word'])\n",
    "\n",
    "    # encoder\n",
    "    params = get_layer('gru')[0](options, params,\n",
    "                                 prefix='encoder',\n",
    "                                 nin=options['dim_word_src'],\n",
    "                                 dim=options['enc_dim'])\n",
    "    params = get_layer('gru')[0](options, params,\n",
    "                                 prefix='encoderr',\n",
    "                                 nin=options['dim_word_src'],\n",
    "                                 dim=options['enc_dim'])\n",
    "    ctxdim = 2 * options['enc_dim']\n",
    "\n",
    "    # init_state of decoder\n",
    "    params = get_layer('ff')[0](options, params,\n",
    "                                prefix='ff_init_state_char',\n",
    "                                nin=ctxdim,\n",
    "                                nout=options['dec_dim'])\n",
    "    params = get_layer('ff')[0](options, params,\n",
    "                                prefix='ff_init_state_word',\n",
    "                                nin=ctxdim,\n",
    "                                nout=options['dec_dim'])\n",
    "\n",
    "    print \"target dictionary size: %d\" % options['n_words']\n",
    "    \n",
    "    # decoder\n",
    "    params = get_layer('two_layer_gru_decoder')[0](options, params,\n",
    "                                                   prefix='decoder',\n",
    "                                                   nin=options['dim_word'],\n",
    "                                                   dim_char=options['dec_dim'],\n",
    "                                                   dim_word=options['dec_dim'],\n",
    "                                                   dimctx=ctxdim)\n",
    "\n",
    "    # readout\n",
    "    params = get_layer('fff')[0](options, params, prefix='ff_logit_rnn',\n",
    "                                 nin1=options['dec_dim'], nin2=options['dec_dim'],\n",
    "                                 nout=options['dim_word'], ortho=False)\n",
    "    params = get_layer('ff')[0](options, params, prefix='ff_logit_prev',\n",
    "                                nin=options['dim_word'],\n",
    "                                nout=options['dim_word'],\n",
    "                                ortho=False)\n",
    "    params = get_layer('ff')[0](options, params, prefix='ff_logit_ctx',\n",
    "                                nin=ctxdim,\n",
    "                                nout=options['dim_word'],\n",
    "                                ortho=False)\n",
    "    params = get_layer('ff')[0](options, params, prefix='ff_logit',\n",
    "                                nin=options['dim_word'],\n",
    "                                nout=options['n_words'])\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def build_model(tparams, options):\n",
    "    opt_ret = OrderedDict()\n",
    "\n",
    "    trng = RandomStreams(numpy.random.RandomState(numpy.random.randint(1024)).randint(numpy.iinfo(numpy.int32).max))\n",
    "    use_noise = theano.shared(numpy.float32(0.))\n",
    "\n",
    "    # description string: #words x #samples\n",
    "    x = tensor.matrix('x', dtype='int64')\n",
    "    x_mask = tensor.matrix('x_mask', dtype='float32')\n",
    "    y = tensor.matrix('y', dtype='int64')\n",
    "    y_mask = tensor.matrix('y_mask', dtype='float32')\n",
    "    x.tag.test_value = numpy.zeros((5, 63), dtype='int64')\n",
    "    x_mask.tag.test_value = numpy.ones((5, 63), dtype='float32')\n",
    "    y.tag.test_value = numpy.zeros((7, 63), dtype='int64')\n",
    "    y_mask.tag.test_value = numpy.ones((7, 63), dtype='float32')\n",
    "\n",
    "    xr = x[::-1]\n",
    "    xr_mask = x_mask[::-1]\n",
    "\n",
    "    n_samples = x.shape[1]\n",
    "    n_timesteps = x.shape[0]\n",
    "    n_timesteps_trg = y.shape[0]\n",
    "\n",
    "    # word embedding for forward RNN (source)\n",
    "    emb = tparams['Wemb'][x.flatten()]\n",
    "    emb = emb.reshape([n_timesteps, n_samples, options['dim_word_src']])\n",
    "\n",
    "    # word embedding for backward RNN (source)\n",
    "    embr = tparams['Wemb'][xr.flatten()]\n",
    "    embr = embr.reshape([n_timesteps, n_samples, options['dim_word_src']])\n",
    "\n",
    "    # pass through gru layer, recurrence here\n",
    "    proj = get_layer('gru')[1](tparams, emb, options,\n",
    "                               prefix='encoder', mask=x_mask)\n",
    "    projr = get_layer('gru')[1](tparams, embr, options,\n",
    "                                prefix='encoderr', mask=xr_mask)\n",
    "\n",
    "    # context\n",
    "    ctx = concatenate([proj, projr[::-1]], axis=proj.ndim-1)\n",
    "\n",
    "    # context mean\n",
    "    ctx_mean = (ctx * x_mask[:, :, None]).sum(0) / x_mask.sum(0)[:, None]\n",
    "\n",
    "    # initial decoder state\n",
    "    init_state_char = get_layer('ff')[1](tparams, ctx_mean, options,\n",
    "                                         prefix='ff_init_state_char', activ='tanh')\n",
    "    init_state_word = get_layer('ff')[1](tparams, ctx_mean, options,\n",
    "                                         prefix='ff_init_state_word', activ='tanh')\n",
    "\n",
    "    # word embedding and shifting for targets\n",
    "    yemb = tparams['Wemb_dec'][y.flatten()]\n",
    "    yemb = yemb.reshape([n_timesteps_trg, n_samples, options['dim_word']])\n",
    "    yemb_shited = tensor.zeros_like(yemb)\n",
    "    yemb_shited = tensor.set_subtensor(yemb_shited[1:], yemb[:-1])\n",
    "    yemb = yemb_shited\n",
    "\n",
    "    char_h, word_h, ctxs, alphas = \\\n",
    "            get_layer('two_layer_gru_decoder')[1](tparams, yemb, options,\n",
    "                                                  prefix='decoder',\n",
    "                                                  mask=y_mask,\n",
    "                                                  context=ctx,\n",
    "                                                  context_mask=x_mask,\n",
    "                                                  one_step=False,\n",
    "                                                  init_state_char=init_state_char,\n",
    "                                                  init_state_word=init_state_word)\n",
    "\n",
    "    opt_ret['dec_alphas'] = alphas\n",
    "\n",
    "    # compute word probabilities\n",
    "    logit_rnn = get_layer('fff')[1](tparams, char_h, word_h, options,\n",
    "                                    prefix='ff_logit_rnn', activ='linear')\n",
    "    logit_prev = get_layer('ff')[1](tparams, yemb, options,\n",
    "                                    prefix='ff_logit_prev', activ='linear')\n",
    "    logit_ctx = get_layer('ff')[1](tparams, ctxs, options,\n",
    "                                   prefix='ff_logit_ctx', activ='linear')\n",
    "    logit = tensor.tanh(logit_rnn + logit_prev + logit_ctx)\n",
    "\n",
    "    if options['use_dropout']:\n",
    "        print 'Using dropout'\n",
    "        logit = dropout_layer(logit, use_noise, trng)\n",
    "\n",
    "    logit = get_layer('ff')[1](tparams, logit, options,\n",
    "                               prefix='ff_logit', activ='linear')\n",
    "    logit_shp = logit.shape\n",
    "    probs = tensor.nnet.softmax(logit.reshape([logit_shp[0]*logit_shp[1], logit_shp[2]]))\n",
    "\n",
    "    # cost\n",
    "    y_flat = y.flatten()\n",
    "    y_flat_idx = tensor.arange(y_flat.shape[0]) * options['n_words'] + y_flat\n",
    "    cost = -tensor.log(probs.flatten()[y_flat_idx])\n",
    "    cost = cost.reshape([y.shape[0], y.shape[1]])\n",
    "    cost = (cost * y_mask).sum(0)\n",
    "\n",
    "    return trng, use_noise, x, x_mask, y, y_mask, opt_ret, cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_sampler(tparams, options, trng, use_noise):\n",
    "    x = tensor.matrix('x', dtype='int64')\n",
    "    xr = x[::-1]\n",
    "\n",
    "    n_timesteps = x.shape[0]\n",
    "    n_samples = x.shape[1]\n",
    "\n",
    "    emb = tparams['Wemb'][x.flatten()]\n",
    "    emb = emb.reshape([n_timesteps, n_samples, options['dim_word_src']])\n",
    "    embr = tparams['Wemb'][xr.flatten()]\n",
    "    embr = embr.reshape([n_timesteps, n_samples, options['dim_word_src']])\n",
    "\n",
    "    proj = get_layer('gru')[1](tparams, emb, options, prefix='encoder')\n",
    "    projr = get_layer('gru')[1](tparams, embr, options, prefix='encoderr')\n",
    "\n",
    "    ctx = concatenate([proj, projr[::-1]], axis=proj.ndim-1)\n",
    "    ctx_mean = ctx.mean(0)\n",
    "\n",
    "    init_state_char = get_layer('ff')[1](tparams, ctx_mean, options,\n",
    "                                         prefix='ff_init_state_char', activ='tanh')\n",
    "    init_state_word = get_layer('ff')[1](tparams, ctx_mean, options,\n",
    "                                         prefix='ff_init_state_word', activ='tanh')\n",
    "\n",
    "    print 'Building f_init...',\n",
    "    outs = [init_state_char, init_state_word, ctx]\n",
    "    f_init = theano.function([x], outs, name='f_init', profile=profile)\n",
    "    print 'Done'\n",
    "\n",
    "    y = tensor.vector('y_sampler', dtype='int64')\n",
    "    init_state_char = tensor.matrix('init_state_char', dtype='float32')\n",
    "    init_state_word = tensor.matrix('init_state_word', dtype='float32')\n",
    "\n",
    "    # if it's the first word, emb should be all zero and it is indicated by -1\n",
    "    yemb = tensor.switch(y[:, None] < 0,\n",
    "                         tensor.alloc(0., 1, tparams['Wemb_dec'].shape[1]),\n",
    "                         tparams['Wemb_dec'][y])\n",
    "\n",
    "    next_state_char, next_state_word, next_ctx, next_alpha = \\\n",
    "            get_layer('two_layer_gru_decoder')[1](tparams, yemb, options,\n",
    "                                                  prefix='decoder',\n",
    "                                                  context=ctx,\n",
    "                                                  mask=None,\n",
    "                                                  one_step=True,\n",
    "                                                  init_state_char=init_state_char,\n",
    "                                                  init_state_word=init_state_word)\n",
    "\n",
    "    logit_rnn = get_layer('fff')[1](tparams,\n",
    "                                    next_state_char,\n",
    "                                    next_state_word,\n",
    "                                    options,\n",
    "                                    prefix='ff_logit_rnn',\n",
    "                                    activ='linear')\n",
    "    logit_prev = get_layer('ff')[1](tparams,\n",
    "                                    yemb,\n",
    "                                    options,\n",
    "                                    prefix='ff_logit_prev',\n",
    "                                    activ='linear')\n",
    "    logit_ctx = get_layer('ff')[1](tparams,\n",
    "                                   next_ctx,\n",
    "                                   options,\n",
    "                                   prefix='ff_logit_ctx',\n",
    "                                   activ='linear')\n",
    "    logit = tensor.tanh(logit_rnn + logit_prev + logit_ctx)\n",
    "\n",
    "    if options['use_dropout']:\n",
    "        print 'Sampling for dropoutted model'\n",
    "        logit = dropout_layer(logit, use_noise, trng)\n",
    "\n",
    "    logit = get_layer('ff')[1](tparams, logit, options,\n",
    "                               prefix='ff_logit',\n",
    "                               activ='linear')\n",
    "    next_probs = tensor.nnet.softmax(logit)\n",
    "    next_sample = trng.multinomial(pvals=next_probs).argmax(1)\n",
    "\n",
    "    # next word probability\n",
    "    print 'Building f_next...',\n",
    "    inps = [y, ctx, init_state_char, init_state_word]\n",
    "    outs = [next_probs, next_sample, next_state_char, next_state_word]\n",
    "    f_next = theano.function(inps, outs, name='f_next', profile=profile)\n",
    "    print 'Done'\n",
    "\n",
    "    return f_init, f_next\n",
    "\n",
    "\n",
    "def gen_sample(tparams, f_init, f_next, x, options, trng=None,\n",
    "               k=1, maxlen=500, stochastic=True, argmax=False):\n",
    "\n",
    "    # k is the beam size we have\n",
    "    if k > 1:\n",
    "        assert not stochastic, \\\n",
    "            'Beam search does not support stochastic sampling'\n",
    "\n",
    "    sample = []\n",
    "    sample_score = []\n",
    "    if stochastic:\n",
    "        sample_score = 0\n",
    "\n",
    "    live_k = 1\n",
    "    dead_k = 0\n",
    "\n",
    "    hyp_samples = [[]] * live_k\n",
    "    hyp_scores = numpy.zeros(live_k).astype('float32')\n",
    "    hyp_states = []\n",
    "\n",
    "    # get initial state of decoder rnn and encoder context\n",
    "    ret = f_init(x)\n",
    "    next_state_char, next_state_word, ctx0 = ret[0], ret[1], ret[2]\n",
    "    next_w = -1 * numpy.ones((1,)).astype('int64')  # bos indicator\n",
    "\n",
    "    for ii in xrange(maxlen):\n",
    "        ctx = numpy.tile(ctx0, [live_k, 1])\n",
    "        inps = [next_w, ctx, next_state_char, next_state_word]\n",
    "        ret = f_next(*inps)\n",
    "        next_p, next_w, next_state_char, next_state_word = ret[0], ret[1], ret[2], ret[3]\n",
    "        if stochastic:\n",
    "            if argmax:\n",
    "                nw = next_p[0].argmax()\n",
    "            else:\n",
    "                nw = next_w[0]\n",
    "            sample.append(nw)\n",
    "            sample_score += next_p[0, nw]\n",
    "            if nw == 0:\n",
    "                break\n",
    "        else:\n",
    "            cand_scores = hyp_scores[:, None] - numpy.log(next_p)\n",
    "            cand_flat = cand_scores.flatten()\n",
    "            ranks_flat = cand_flat.argsort()[:(k-dead_k)]\n",
    "\n",
    "            voc_size = next_p.shape[1]\n",
    "            trans_indices = ranks_flat / voc_size\n",
    "            word_indices = ranks_flat % voc_size\n",
    "            costs = cand_flat[ranks_flat]\n",
    "\n",
    "            new_hyp_samples = []\n",
    "            new_hyp_scores = numpy.zeros(k-dead_k).astype('float32')\n",
    "            new_hyp_states_char = []\n",
    "            new_hyp_states_word = []\n",
    "\n",
    "            for idx, [ti, wi] in enumerate(zip(trans_indices, word_indices)):\n",
    "                new_hyp_samples.append(hyp_samples[ti]+[wi])\n",
    "                new_hyp_scores[idx] = copy.copy(costs[idx])\n",
    "                new_hyp_states_char.append(copy.copy(next_state_char[ti]))\n",
    "                new_hyp_states_word.append(copy.copy(next_state_word[ti]))\n",
    "\n",
    "            # check the finished samples\n",
    "            new_live_k = 0\n",
    "            hyp_samples = []\n",
    "            hyp_scores = []\n",
    "            hyp_states_char = []\n",
    "            hyp_states_word = []\n",
    "\n",
    "            for idx in xrange(len(new_hyp_samples)):\n",
    "                if new_hyp_samples[idx][-1] == 0:\n",
    "                    sample.append(new_hyp_samples[idx])\n",
    "                    sample_score.append(new_hyp_scores[idx])\n",
    "                    dead_k += 1\n",
    "                else:\n",
    "                    new_live_k += 1\n",
    "                    hyp_samples.append(new_hyp_samples[idx])\n",
    "                    hyp_scores.append(new_hyp_scores[idx])\n",
    "                    hyp_states_char.append(new_hyp_states_char[idx])\n",
    "                    hyp_states_word.append(new_hyp_states_word[idx])\n",
    "            hyp_scores = numpy.array(hyp_scores)\n",
    "            live_k = new_live_k\n",
    "\n",
    "            if new_live_k < 1:\n",
    "                break\n",
    "            if dead_k >= k:\n",
    "                break\n",
    "\n",
    "            next_w = numpy.array([w[-1] for w in hyp_samples])\n",
    "            next_state_char = numpy.array(hyp_states_char)\n",
    "            next_state_word = numpy.array(hyp_states_word)\n",
    "\n",
    "    if not stochastic:\n",
    "        # dump every remaining one\n",
    "        if live_k > 0:\n",
    "            for idx in xrange(live_k):\n",
    "                sample.append(hyp_samples[idx])\n",
    "                sample_score.append(hyp_scores[idx])\n",
    "\n",
    "    return sample, sample_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train function from nmt file\n",
    "# https://github.com/nyu-dl/dl4mt-c2c/blob/master/bpe2char/nmt.py\n",
    "\n",
    "\n",
    "def train(\n",
    "      dim_word=100,\n",
    "      dim_word_src=200,\n",
    "      enc_dim=1000,\n",
    "      dec_dim=1000,  # the number of LSTM units\n",
    "      patience=-1,  # early stopping patience\n",
    "      max_epochs=5000,\n",
    "      finish_after=-1,  # finish after this many updates\n",
    "      decay_c=0.,  # L2 regularization penalty\n",
    "      alpha_c=0.,  # alignment regularization\n",
    "      clip_c=-1.,  # gradient clipping threshold\n",
    "      lrate=0.01,  # learning rate\n",
    "      n_words_src=100000,  # source vocabulary size\n",
    "      n_words=100000,  # target vocabulary size\n",
    "      maxlen=100,  # maximum length of the description\n",
    "      maxlen_trg=None,  # maximum length of the description\n",
    "      maxlen_sample=1000,\n",
    "      optimizer='rmsprop',\n",
    "      batch_size=16,\n",
    "      valid_batch_size=16,\n",
    "      sort_size=20,\n",
    "      save_path=None,\n",
    "      save_file_name='model',\n",
    "      save_best_models=0,\n",
    "      dispFreq=100,\n",
    "      validFreq=100,\n",
    "      saveFreq=1000,   # save the parameters after every saveFreq updates\n",
    "      sampleFreq=-1,\n",
    "      verboseFreq=10000,\n",
    "      datasets=[\n",
    "          'data/lisatmp3/chokyun/europarl/europarl-v7.fr-en.en.tok',\n",
    "          '/data/lisatmp3/chokyun/europarl/europarl-v7.fr-en.fr.tok'],\n",
    "      valid_datasets=['../data/dev/newstest2011.en.tok',\n",
    "                      '../data/dev/newstest2011.fr.tok'],\n",
    "      dictionaries=[\n",
    "          '/data/lisatmp3/chokyun/europarl/europarl-v7.fr-en.en.tok.pkl',\n",
    "          '/data/lisatmp3/chokyun/europarl/europarl-v7.fr-en.fr.tok.pkl'],\n",
    "      source_word_level=0,\n",
    "      target_word_level=0,\n",
    "      use_dropout=False,\n",
    "      re_load=False,\n",
    "      re_load_old_setting=False,\n",
    "      uidx=None,\n",
    "      eidx=None,\n",
    "      cidx=None,\n",
    "      layers=None,\n",
    "      save_every_saveFreq=0,\n",
    "      save_burn_in=20000,\n",
    "      use_bpe=0,\n",
    "      gru='gru',\n",
    "      init_params=None,\n",
    "      build_model=None,\n",
    "      build_sampler=None,\n",
    "      gen_sample=None,\n",
    "      **kwargs\n",
    "    ):\n",
    "\n",
    "    if gru not in \"gru lngru\".split():\n",
    "        raise\n",
    "\n",
    "    print \"GRU:\", gru\n",
    "\n",
    "    if maxlen_trg is None:\n",
    "        maxlen_trg = maxlen * 10\n",
    "    # Model options\n",
    "    model_options = locals().copy()\n",
    "    del model_options['init_params']\n",
    "    del model_options['build_model']\n",
    "    del model_options['build_sampler']\n",
    "    del model_options['gen_sample']\n",
    "\n",
    "    # load dictionaries and invert them\n",
    "    worddicts = [None] * len(dictionaries)\n",
    "    worddicts_r = [None] * len(dictionaries)\n",
    "    for ii, dd in enumerate(dictionaries):\n",
    "        with open(dd, 'rb') as f:\n",
    "            worddicts[ii] = cPickle.load(f)\n",
    "        worddicts_r[ii] = dict()\n",
    "        for kk, vv in worddicts[ii].iteritems():\n",
    "            worddicts_r[ii][vv] = kk\n",
    "\n",
    "    print 'Building model'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    file_name = '%s%s.npz' % (save_path, save_file_name)\n",
    "    best_file_name = '%s%s.best.npz' % (save_path, save_file_name)\n",
    "    opt_file_name = '%s%s%s.npz' % (save_path, save_file_name, '.grads')\n",
    "    best_opt_file_name = '%s%s%s.best.npz' % (save_path, save_file_name, '.grads')\n",
    "    model_name = '%s%s.pkl' % (save_path, save_file_name)\n",
    "    params = init_params(model_options)\n",
    "    cPickle.dump(model_options, open(model_name, 'wb'))\n",
    "    history_errs = []\n",
    "\n",
    "    # reload options\n",
    "    if re_load and os.path.exists(file_name):\n",
    "        print 'You are reloading your experiment.. do not panic dude..'\n",
    "        if re_load_old_setting:\n",
    "            with open(model_name, 'rb') as f:\n",
    "                model_options = cPickle.load(f)\n",
    "        params = load_params(file_name, params)\n",
    "        # reload history\n",
    "        model = numpy.load(file_name)\n",
    "        history_errs = list(model['history_errs'])\n",
    "        if uidx is None:\n",
    "            uidx = model['uidx']\n",
    "        if eidx is None:\n",
    "            eidx = model['eidx']\n",
    "        if cidx is None:\n",
    "            cidx = model['cidx']\n",
    "    else:\n",
    "        if uidx is None:\n",
    "            uidx = 0\n",
    "        if eidx is None:\n",
    "            eidx = 0\n",
    "        if cidx is None:\n",
    "            cidx = 0\n",
    "\n",
    "    print 'Loading data'\n",
    "    train = TextIterator(source=datasets[0],\n",
    "                         target=datasets[1],\n",
    "                         source_dict=dictionaries[0],\n",
    "                         target_dict=dictionaries[1],\n",
    "                         n_words_source=n_words_src,\n",
    "                         n_words_target=n_words,\n",
    "                         source_word_level=source_word_level,\n",
    "                         target_word_level=target_word_level,\n",
    "                         batch_size=batch_size,\n",
    "                         sort_size=sort_size)\n",
    "    valid = TextIterator(source=valid_datasets[0],\n",
    "                         target=valid_datasets[1],\n",
    "                         source_dict=dictionaries[0],\n",
    "                         target_dict=dictionaries[1],\n",
    "                         n_words_source=n_words_src,\n",
    "                         n_words_target=n_words,\n",
    "                         source_word_level=source_word_level,\n",
    "                         target_word_level=target_word_level,\n",
    "                         batch_size=valid_batch_size,\n",
    "                         sort_size=sort_size)\n",
    "\n",
    "    # create shared variables for parameters\n",
    "    tparams = init_tparams(params)\n",
    "\n",
    "    trng, use_noise, \\\n",
    "        x, x_mask, y, y_mask, \\\n",
    "        opt_ret, \\\n",
    "        cost = \\\n",
    "        build_model(tparams, model_options)\n",
    "    inps = [x, x_mask, y, y_mask]\n",
    "\n",
    "    print 'Building sampler...\\n',\n",
    "    f_init, f_next = build_sampler(tparams, model_options, trng, use_noise)\n",
    "    #print 'Done'\n",
    "\n",
    "    # before any regularizer\n",
    "    print 'Building f_log_probs...',\n",
    "    f_log_probs = theano.function(inps, cost, profile=profile)\n",
    "    print 'Done'\n",
    "    if re_load:\n",
    "        use_noise.set_value(0.)\n",
    "        valid_errs = pred_probs(f_log_probs, prepare_data,\n",
    "                                model_options, valid, verboseFreq=verboseFreq)\n",
    "        valid_err = valid_errs.mean()\n",
    "\n",
    "        if numpy.isnan(valid_err):\n",
    "            import ipdb\n",
    "            ipdb.set_trace()\n",
    "\n",
    "        print 'Reload sanity check: Valid ', valid_err\n",
    "\n",
    "    cost = cost.mean()\n",
    "\n",
    "    # apply L2 regularization on weights\n",
    "    if decay_c > 0.:\n",
    "        decay_c = theano.shared(numpy.float32(decay_c), name='decay_c')\n",
    "        weight_decay = 0.\n",
    "        for kk, vv in tparams.iteritems():\n",
    "            weight_decay += (vv ** 2).sum()\n",
    "        weight_decay *= decay_c\n",
    "        cost += weight_decay\n",
    "\n",
    "    # regularize the alpha weights\n",
    "    if alpha_c > 0. and not model_options['decoder'].endswith('simple'):\n",
    "        alpha_c = theano.shared(numpy.float32(alpha_c), name='alpha_c')\n",
    "        alpha_reg = alpha_c * (\n",
    "            (tensor.cast(y_mask.sum(0) // x_mask.sum(0), 'float32')[:, None] -\n",
    "             opt_ret['dec_alphas'].sum(0))**2).sum(1).mean()\n",
    "        cost += alpha_reg\n",
    "\n",
    "    # after all regularizers - compile the computational graph for cost\n",
    "    print 'Building f_cost...',\n",
    "    f_cost = theano.function(inps, cost, profile=profile)\n",
    "    print 'Done'\n",
    "\n",
    "    print 'Computing gradient...',\n",
    "    grads = tensor.grad(cost, wrt=itemlist(tparams))\n",
    "    print 'Done'\n",
    "\n",
    "    if clip_c > 0:\n",
    "        grads, not_finite, clipped = gradient_clipping(grads, tparams, clip_c)\n",
    "    else:\n",
    "        not_finite = 0\n",
    "        clipped = 0\n",
    "\n",
    "    # compile the optimizer, the actual computational graph is compiled here\n",
    "    lr = tensor.scalar(name='lr')\n",
    "    print 'Building optimizers...',\n",
    "    if re_load and os.path.exists(file_name):\n",
    "        if clip_c > 0:\n",
    "            f_grad_shared, f_update, toptparams = eval(optimizer)(lr, tparams, grads, inps, cost=cost,\n",
    "                                                                  not_finite=not_finite, clipped=clipped,\n",
    "                                                                  file_name=opt_file_name)\n",
    "        else:\n",
    "            f_grad_shared, f_update, toptparams = eval(optimizer)(lr, tparams, grads, inps, cost=cost,\n",
    "                                                                  file_name=opt_file_name)\n",
    "    else:\n",
    "        if clip_c > 0:\n",
    "            f_grad_shared, f_update, toptparams = eval(optimizer)(lr, tparams, grads, inps, cost=cost,\n",
    "                                                                  not_finite=not_finite, clipped=clipped)\n",
    "        else:\n",
    "            f_grad_shared, f_update, toptparams = eval(optimizer)(lr, tparams, grads, inps, cost=cost)\n",
    "    print 'Done'\n",
    "\n",
    "    print 'Optimization'\n",
    "    best_p = None\n",
    "    bad_counter = 0\n",
    "\n",
    "    if validFreq == -1:\n",
    "        validFreq = len(train[0]) / batch_size\n",
    "    if saveFreq == -1:\n",
    "        saveFreq = len(train[0]) / batch_size\n",
    "\n",
    "    # Training loop\n",
    "    ud_start = time.time()\n",
    "    estop = False\n",
    "\n",
    "    if re_load:\n",
    "        print \"Checkpointed minibatch number: %d\" % cidx\n",
    "        for cc in xrange(cidx):\n",
    "            if numpy.mod(cc, 1000)==0:\n",
    "                print \"Jumping [%d / %d] examples\" % (cc, cidx)\n",
    "            train.next()\n",
    "\n",
    "    for epoch in xrange(max_epochs):\n",
    "        time0 = time.time()\n",
    "        n_samples = 0\n",
    "        NaN_grad_cnt = 0\n",
    "        NaN_cost_cnt = 0\n",
    "        clipped_cnt = 0\n",
    "        if re_load:\n",
    "            re_load = 0\n",
    "        else:\n",
    "            cidx = 0\n",
    "\n",
    "        for x, y in train:\n",
    "            cidx += 1\n",
    "            uidx += 1\n",
    "            use_noise.set_value(1.)\n",
    "\n",
    "            x, x_mask, y, y_mask, n_x = prepare_data(x, y, maxlen=maxlen,\n",
    "                                                     maxlen_trg=maxlen_trg,\n",
    "                                                     n_words_src=n_words_src,\n",
    "                                                     n_words=n_words)\n",
    "            if x is None:\n",
    "                print 'Minibatch with zero sample under length ', maxlen\n",
    "                uidx -= 1\n",
    "                uidx = max(uidx, 0)\n",
    "                continue\n",
    "\n",
    "            n_samples += n_x\n",
    "\n",
    "            # compute cost, grads and copy grads to shared variables\n",
    "            if clip_c > 0:\n",
    "                cost, not_finite, clipped = f_grad_shared(x, x_mask, y, y_mask)\n",
    "            else:\n",
    "                cost = f_grad_shared(x, x_mask, y, y_mask)\n",
    "\n",
    "            if clipped:\n",
    "                clipped_cnt += 1\n",
    "\n",
    "            # check for bad numbers, usually we remove non-finite elements\n",
    "            # and continue training - but not done here\n",
    "            if numpy.isnan(cost) or numpy.isinf(cost):\n",
    "                NaN_cost_cnt += 1\n",
    "\n",
    "            if not_finite:\n",
    "                NaN_grad_cnt += 1\n",
    "                continue\n",
    "\n",
    "            # do the update on parameters\n",
    "            f_update(lrate)\n",
    "\n",
    "            if numpy.isnan(cost) or numpy.isinf(cost):\n",
    "                continue\n",
    "\n",
    "            if float(NaN_grad_cnt) > max_epochs * 0.5 or float(NaN_cost_cnt) > max_epochs * 0.5:\n",
    "                print 'Too many NaNs, abort training'\n",
    "                return 1., 1., 1.\n",
    "\n",
    "            # verbose\n",
    "            if numpy.mod(uidx, dispFreq) == 0:\n",
    "                ud = time.time() - ud_start\n",
    "                wps = n_samples / float(time.time() - time0)\n",
    "                print 'Epoch ', eidx, 'Update ', uidx, 'Cost ', cost, 'NaN_in_grad', NaN_grad_cnt,\\\n",
    "                      'NaN_in_cost', NaN_cost_cnt, 'Gradient_clipped', clipped_cnt, 'UD ', ud, \"%.2f sentence/s\" % wps\n",
    "                ud_start = time.time()\n",
    "\n",
    "            # generate some samples with the model and display them\n",
    "            if numpy.mod(uidx, sampleFreq) == 0 and sampleFreq != -1:\n",
    "                # FIXME: random selection?\n",
    "                for jj in xrange(numpy.minimum(5, x.shape[1])):\n",
    "                    stochastic = True\n",
    "                    use_noise.set_value(0.)\n",
    "                    sample, score = gen_sample(tparams, f_init, f_next,\n",
    "                                               x[:, jj][:, None],\n",
    "                                               model_options, trng=trng, k=1,\n",
    "                                               maxlen=maxlen_sample,\n",
    "                                               stochastic=stochastic,\n",
    "                                               argmax=False)\n",
    "                    print\n",
    "                    print 'Source ', jj, ': ',\n",
    "                    if source_word_level:\n",
    "                        for vv in x[:, jj]:\n",
    "                            if vv == 0:\n",
    "                                break\n",
    "                            if vv in worddicts_r[0]:\n",
    "                                if use_bpe:\n",
    "                                    print (worddicts_r[0][vv]).replace('@@', ''),\n",
    "                                else:\n",
    "                                    print worddicts_r[0][vv],\n",
    "                            else:\n",
    "                                print 'UNK',\n",
    "                        print\n",
    "                    else:\n",
    "                        source_ = []\n",
    "                        for vv in x[:, jj]:\n",
    "                            if vv == 0:\n",
    "                                break\n",
    "                            if vv in worddicts_r[0]:\n",
    "                                source_.append(worddicts_r[0][vv])\n",
    "                            else:\n",
    "                                source_.append('UNK')\n",
    "                        print \"\".join(source_)\n",
    "                    print 'Truth ', jj, ' : ',\n",
    "                    if target_word_level:\n",
    "                        for vv in y[:, jj]:\n",
    "                            if vv == 0:\n",
    "                                break\n",
    "                            if vv in worddicts_r[1]:\n",
    "                                if use_bpe:\n",
    "                                    print (worddicts_r[1][vv]).replace('@@', ''),\n",
    "                                else:\n",
    "                                    print worddicts_r[1][vv],\n",
    "                            else:\n",
    "                                print 'UNK',\n",
    "                        print\n",
    "                    else:\n",
    "                        truth_ = []\n",
    "                        for vv in y[:, jj]:\n",
    "                            if vv == 0:\n",
    "                                break\n",
    "                            if vv in worddicts_r[1]:\n",
    "                                truth_.append(worddicts_r[1][vv])\n",
    "                            else:\n",
    "                                truth_.append('UNK')\n",
    "                        print \"\".join(truth_)\n",
    "                    print 'Sample ', jj, ': ',\n",
    "                    if stochastic:\n",
    "                        ss = sample\n",
    "                    else:\n",
    "                        score = score / numpy.array([len(s) for s in sample])\n",
    "                        ss = sample[score.argmin()]\n",
    "                    if target_word_level:\n",
    "                        for vv in ss:\n",
    "                            if vv == 0:\n",
    "                                break\n",
    "                            if vv in worddicts_r[1]:\n",
    "                                if use_bpe:\n",
    "                                    print (worddicts_r[1][vv]).replace('@@', ''),\n",
    "                                else:\n",
    "                                    print worddicts_r[1][vv],\n",
    "                            else:\n",
    "                                print 'UNK',\n",
    "                        print\n",
    "                    else:\n",
    "                        sample_ = []\n",
    "                        for vv in ss:\n",
    "                            if vv == 0:\n",
    "                                break\n",
    "                            if vv in worddicts_r[1]:\n",
    "                                sample_.append(worddicts_r[1][vv])\n",
    "                            else:\n",
    "                                sample_.append('UNK')\n",
    "                        print \"\".join(sample_)\n",
    "                    print\n",
    "\n",
    "            # validate model on validation set and early stop if necessary\n",
    "            if numpy.mod(uidx, validFreq) == 0:\n",
    "                use_noise.set_value(0.)\n",
    "                valid_errs = pred_probs(f_log_probs, prepare_data,\n",
    "                                        model_options, valid, verboseFreq=verboseFreq)\n",
    "                valid_err = valid_errs.mean()\n",
    "                history_errs.append(valid_err)\n",
    "\n",
    "                if uidx == 0 or valid_err <= numpy.array(history_errs).min():\n",
    "                    best_p = unzip(tparams)\n",
    "                    best_optp = unzip(toptparams)\n",
    "                    bad_counter = 0\n",
    "\n",
    "                if saveFreq != validFreq and save_best_models:\n",
    "                    numpy.savez(best_file_name, history_errs=history_errs, uidx=uidx, eidx=eidx,\n",
    "                                cidx=cidx, **best_p)\n",
    "                    numpy.savez(best_opt_file_name, **best_optp)\n",
    "\n",
    "                if len(history_errs) > patience and valid_err >= \\\n",
    "                        numpy.array(history_errs)[:-patience].min() and patience != -1:\n",
    "                    bad_counter += 1\n",
    "                    if bad_counter > patience:\n",
    "                        print 'Early Stop!'\n",
    "                        estop = True\n",
    "                        break\n",
    "\n",
    "                if numpy.isnan(valid_err):\n",
    "                    import ipdb\n",
    "                    ipdb.set_trace()\n",
    "\n",
    "                print 'Valid ', valid_err\n",
    "\n",
    "            # save the best model so far\n",
    "            if numpy.mod(uidx, saveFreq) == 0:\n",
    "                print 'Saving...',\n",
    "\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.mkdir(save_path)\n",
    "\n",
    "                params = unzip(tparams)\n",
    "                optparams = unzip(toptparams)\n",
    "                numpy.savez(file_name, history_errs=history_errs, uidx=uidx, eidx=eidx,\n",
    "                            cidx=cidx, **params)\n",
    "                numpy.savez(opt_file_name, **optparams)\n",
    "\n",
    "                if save_every_saveFreq and (uidx >= save_burn_in):\n",
    "                    this_file_name = '%s%s.%d.npz' % (save_path, save_file_name, uidx)\n",
    "                    this_opt_file_name = '%s%s%s.%d.npz' % (save_path, save_file_name, '.grads', uidx)\n",
    "                    numpy.savez(this_file_name, history_errs=history_errs, uidx=uidx, eidx=eidx,\n",
    "                                cidx=cidx, **params)\n",
    "                    numpy.savez(this_opt_file_name, history_errs=history_errs, uidx=uidx, eidx=eidx,\n",
    "                                cidx=cidx, **params)\n",
    "                    if best_p is not None and saveFreq != validFreq:\n",
    "                        this_best_file_name = '%s%s.%d.best.npz' % (save_path, save_file_name, uidx)\n",
    "                        numpy.savez(this_best_file_name, history_errs=history_errs, uidx=uidx, eidx=eidx,\n",
    "                                    cidx=cidx, **best_p)\n",
    "                print 'Done...',\n",
    "                print 'Saved to %s' % file_name\n",
    "\n",
    "            # finish after this many updates\n",
    "            if uidx >= finish_after and finish_after != -1:\n",
    "                print 'Finishing after %d iterations!' % uidx\n",
    "                estop = True\n",
    "                break\n",
    "\n",
    "        print 'Seen %d samples' % n_samples\n",
    "        eidx += 1\n",
    "\n",
    "        if estop:\n",
    "            break\n",
    "\n",
    "    use_noise.set_value(0.)\n",
    "    valid_err = pred_probs(f_log_probs, prepare_data,\n",
    "                           model_options, valid).mean()\n",
    "\n",
    "    print 'Valid ', valid_err\n",
    "\n",
    "    params = unzip(tparams)\n",
    "    optparams = unzip(toptparams)\n",
    "    file_name = '%s%s.%d.npz' % (save_path, save_file_name, uidx)\n",
    "    opt_file_name = '%s%s%s.%d.npz' % (save_path, save_file_name, '.grads', uidx)\n",
    "    numpy.savez(file_name, history_errs=history_errs, uidx=uidx, eidx=eidx, cidx=cidx, **params)\n",
    "    numpy.savez(opt_file_name, **optparams)\n",
    "    if best_p is not None and saveFreq != validFreq:\n",
    "        best_file_name = '%s%s.%d.best.npz' % (save_path, save_file_name, uidx)\n",
    "        best_opt_file_name = '%s%s%s.%d.best.npz' % (save_path, save_file_name, '.grads',uidx)\n",
    "        numpy.savez(best_file_name, history_errs=history_errs, uidx=uidx, eidx=eidx, cidx=cidx, **best_p)\n",
    "        numpy.savez(best_opt_file_name, **best_optp)\n",
    "\n",
    "    return valid_err"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
