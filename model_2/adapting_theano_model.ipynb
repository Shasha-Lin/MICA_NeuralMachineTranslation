{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bilingual bpe2char model in pytorch\n",
    "# based on theano code https://github.com/nyu-dl/dl4mt-c2c/blob/master/bpe2char/char_base.py\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "##### ORIGINAL CODE #####\n",
    "'''\n",
    "Build a simple neural language model using GRU units\n",
    "'''\n",
    "import theano\n",
    "from theano import tensor\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "import cPickle\n",
    "import numpy\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from collections import OrderedDict\n",
    "from mixer import *\n",
    "\n",
    "\n",
    "def init_params(options):\n",
    "    params = OrderedDict()\n",
    "\n",
    "    print \"source dictionary size: %d\" % options['n_words_src']\n",
    "    # embedding\n",
    "    params['Wemb'] = norm_weight(options['n_words_src'], options['dim_word_src'])\n",
    "    params['Wemb_dec'] = norm_weight(options['n_words'], options['dim_word'])\n",
    "\n",
    "    # encoder\n",
    "    params = get_layer('gru')[0](options, params,\n",
    "                                 prefix='encoder',\n",
    "                                 nin=options['dim_word_src'],\n",
    "                                 dim=options['enc_dim'])\n",
    "    params = get_layer('gru')[0](options, params,\n",
    "                                 prefix='encoderr',\n",
    "                                 nin=options['dim_word_src'],\n",
    "                                 dim=options['enc_dim'])\n",
    "    ctxdim = 2 * options['enc_dim']\n",
    "\n",
    "    # init_state of decoder\n",
    "    params = get_layer('ff')[0](options, params,\n",
    "                                prefix='ff_init_state_char',\n",
    "                                nin=ctxdim,\n",
    "                                nout=options['dec_dim'])\n",
    "    params = get_layer('ff')[0](options, params,\n",
    "                                prefix='ff_init_state_word',\n",
    "                                nin=ctxdim,\n",
    "                                nout=options['dec_dim'])\n",
    "\n",
    "    print \"target dictionary size: %d\" % options['n_words']\n",
    "    # decoder\n",
    "    params = get_layer('two_layer_gru_decoder')[0](options, params,\n",
    "                                                   prefix='decoder',\n",
    "                                                   nin=options['dim_word'],\n",
    "                                                   dim_char=options['dec_dim'],\n",
    "                                                   dim_word=options['dec_dim'],\n",
    "                                                   dimctx=ctxdim)\n",
    "\n",
    "    # readout\n",
    "    params = get_layer('fff')[0](options, params, prefix='ff_logit_rnn',\n",
    "                                 nin1=options['dec_dim'], nin2=options['dec_dim'],\n",
    "                                 nout=options['dim_word'], ortho=False)\n",
    "    params = get_layer('ff')[0](options, params, prefix='ff_logit_prev',\n",
    "                                nin=options['dim_word'],\n",
    "                                nout=options['dim_word'],\n",
    "                                ortho=False)\n",
    "    params = get_layer('ff')[0](options, params, prefix='ff_logit_ctx',\n",
    "                                nin=ctxdim,\n",
    "                                nout=options['dim_word'],\n",
    "                                ortho=False)\n",
    "    params = get_layer('ff')[0](options, params, prefix='ff_logit',\n",
    "                                nin=options['dim_word'],\n",
    "                                nout=options['n_words'])\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def build_model(tparams, options):\n",
    "    opt_ret = OrderedDict()\n",
    "\n",
    "    trng = RandomStreams(numpy.random.RandomState(numpy.random.randint(1024)).randint(numpy.iinfo(numpy.int32).max))\n",
    "    use_noise = theano.shared(numpy.float32(0.))\n",
    "\n",
    "    # description string: #words x #samples\n",
    "    x = tensor.matrix('x', dtype='int64')\n",
    "    x_mask = tensor.matrix('x_mask', dtype='float32')\n",
    "    y = tensor.matrix('y', dtype='int64')\n",
    "    y_mask = tensor.matrix('y_mask', dtype='float32')\n",
    "    x.tag.test_value = numpy.zeros((5, 63), dtype='int64')\n",
    "    x_mask.tag.test_value = numpy.ones((5, 63), dtype='float32')\n",
    "    y.tag.test_value = numpy.zeros((7, 63), dtype='int64')\n",
    "    y_mask.tag.test_value = numpy.ones((7, 63), dtype='float32')\n",
    "\n",
    "    xr = x[::-1]\n",
    "    xr_mask = x_mask[::-1]\n",
    "\n",
    "    n_samples = x.shape[1]\n",
    "    n_timesteps = x.shape[0]\n",
    "    n_timesteps_trg = y.shape[0]\n",
    "\n",
    "    # word embedding for forward RNN (source)\n",
    "    emb = tparams['Wemb'][x.flatten()]\n",
    "    emb = emb.reshape([n_timesteps, n_samples, options['dim_word_src']])\n",
    "\n",
    "    # word embedding for backward RNN (source)\n",
    "    embr = tparams['Wemb'][xr.flatten()]\n",
    "    embr = embr.reshape([n_timesteps, n_samples, options['dim_word_src']])\n",
    "\n",
    "    # pass through gru layer, recurrence here\n",
    "    proj = get_layer('gru')[1](tparams, emb, options,\n",
    "                               prefix='encoder', mask=x_mask)\n",
    "    projr = get_layer('gru')[1](tparams, embr, options,\n",
    "                                prefix='encoderr', mask=xr_mask)\n",
    "\n",
    "    # context\n",
    "    ctx = concatenate([proj, projr[::-1]], axis=proj.ndim-1)\n",
    "\n",
    "    # context mean\n",
    "    ctx_mean = (ctx * x_mask[:, :, None]).sum(0) / x_mask.sum(0)[:, None]\n",
    "\n",
    "    # initial decoder state\n",
    "    init_state_char = get_layer('ff')[1](tparams, ctx_mean, options,\n",
    "                                         prefix='ff_init_state_char', activ='tanh')\n",
    "    init_state_word = get_layer('ff')[1](tparams, ctx_mean, options,\n",
    "                                         prefix='ff_init_state_word', activ='tanh')\n",
    "\n",
    "    # word embedding and shifting for targets\n",
    "    yemb = tparams['Wemb_dec'][y.flatten()]\n",
    "    yemb = yemb.reshape([n_timesteps_trg, n_samples, options['dim_word']])\n",
    "    yemb_shited = tensor.zeros_like(yemb)\n",
    "    yemb_shited = tensor.set_subtensor(yemb_shited[1:], yemb[:-1])\n",
    "    yemb = yemb_shited\n",
    "\n",
    "    char_h, word_h, ctxs, alphas = \\\n",
    "            get_layer('two_layer_gru_decoder')[1](tparams, yemb, options,\n",
    "                                                  prefix='decoder',\n",
    "                                                  mask=y_mask,\n",
    "                                                  context=ctx,\n",
    "                                                  context_mask=x_mask,\n",
    "                                                  one_step=False,\n",
    "                                                  init_state_char=init_state_char,\n",
    "                                                  init_state_word=init_state_word)\n",
    "\n",
    "    opt_ret['dec_alphas'] = alphas\n",
    "\n",
    "    # compute word probabilities\n",
    "    logit_rnn = get_layer('fff')[1](tparams, char_h, word_h, options,\n",
    "                                    prefix='ff_logit_rnn', activ='linear')\n",
    "    logit_prev = get_layer('ff')[1](tparams, yemb, options,\n",
    "                                    prefix='ff_logit_prev', activ='linear')\n",
    "    logit_ctx = get_layer('ff')[1](tparams, ctxs, options,\n",
    "                                   prefix='ff_logit_ctx', activ='linear')\n",
    "    logit = tensor.tanh(logit_rnn + logit_prev + logit_ctx)\n",
    "\n",
    "    if options['use_dropout']:\n",
    "        print 'Using dropout'\n",
    "        logit = dropout_layer(logit, use_noise, trng)\n",
    "\n",
    "    logit = get_layer('ff')[1](tparams, logit, options,\n",
    "                               prefix='ff_logit', activ='linear')\n",
    "    logit_shp = logit.shape\n",
    "    probs = tensor.nnet.softmax(logit.reshape([logit_shp[0]*logit_shp[1], logit_shp[2]]))\n",
    "\n",
    "    # cost\n",
    "    y_flat = y.flatten()\n",
    "    y_flat_idx = tensor.arange(y_flat.shape[0]) * options['n_words'] + y_flat\n",
    "    cost = -tensor.log(probs.flatten()[y_flat_idx])\n",
    "    cost = cost.reshape([y.shape[0], y.shape[1]])\n",
    "    cost = (cost * y_mask).sum(0)\n",
    "\n",
    "    return trng, use_noise, x, x_mask, y, y_mask, opt_ret, cost\n",
    "\n",
    "\n",
    "def build_sampler(tparams, options, trng, use_noise):\n",
    "    x = tensor.matrix('x', dtype='int64')\n",
    "    xr = x[::-1]\n",
    "\n",
    "    n_timesteps = x.shape[0]\n",
    "    n_samples = x.shape[1]\n",
    "\n",
    "    emb = tparams['Wemb'][x.flatten()]\n",
    "    emb = emb.reshape([n_timesteps, n_samples, options['dim_word_src']])\n",
    "    embr = tparams['Wemb'][xr.flatten()]\n",
    "    embr = embr.reshape([n_timesteps, n_samples, options['dim_word_src']])\n",
    "\n",
    "    proj = get_layer('gru')[1](tparams, emb, options, prefix='encoder')\n",
    "    projr = get_layer('gru')[1](tparams, embr, options, prefix='encoderr')\n",
    "\n",
    "    ctx = concatenate([proj, projr[::-1]], axis=proj.ndim-1)\n",
    "    ctx_mean = ctx.mean(0)\n",
    "\n",
    "    init_state_char = get_layer('ff')[1](tparams, ctx_mean, options,\n",
    "                                         prefix='ff_init_state_char', activ='tanh')\n",
    "    init_state_word = get_layer('ff')[1](tparams, ctx_mean, options,\n",
    "                                         prefix='ff_init_state_word', activ='tanh')\n",
    "\n",
    "    print 'Building f_init...',\n",
    "    outs = [init_state_char, init_state_word, ctx]\n",
    "    f_init = theano.function([x], outs, name='f_init', profile=profile)\n",
    "    print 'Done'\n",
    "\n",
    "    y = tensor.vector('y_sampler', dtype='int64')\n",
    "    init_state_char = tensor.matrix('init_state_char', dtype='float32')\n",
    "    init_state_word = tensor.matrix('init_state_word', dtype='float32')\n",
    "\n",
    "    # if it's the first word, emb should be all zero and it is indicated by -1\n",
    "    yemb = tensor.switch(y[:, None] < 0,\n",
    "                         tensor.alloc(0., 1, tparams['Wemb_dec'].shape[1]),\n",
    "                         tparams['Wemb_dec'][y])\n",
    "\n",
    "    next_state_char, next_state_word, next_ctx, next_alpha = \\\n",
    "            get_layer('two_layer_gru_decoder')[1](tparams, yemb, options,\n",
    "                                                  prefix='decoder',\n",
    "                                                  context=ctx,\n",
    "                                                  mask=None,\n",
    "                                                  one_step=True,\n",
    "                                                  init_state_char=init_state_char,\n",
    "                                                  init_state_word=init_state_word)\n",
    "\n",
    "    logit_rnn = get_layer('fff')[1](tparams,\n",
    "                                    next_state_char,\n",
    "                                    next_state_word,\n",
    "                                    options,\n",
    "                                    prefix='ff_logit_rnn',\n",
    "                                    activ='linear')\n",
    "    logit_prev = get_layer('ff')[1](tparams,\n",
    "                                    yemb,\n",
    "                                    options,\n",
    "                                    prefix='ff_logit_prev',\n",
    "                                    activ='linear')\n",
    "    logit_ctx = get_layer('ff')[1](tparams,\n",
    "                                   next_ctx,\n",
    "                                   options,\n",
    "                                   prefix='ff_logit_ctx',\n",
    "                                   activ='linear')\n",
    "    logit = tensor.tanh(logit_rnn + logit_prev + logit_ctx)\n",
    "\n",
    "    if options['use_dropout']:\n",
    "        print 'Sampling for dropoutted model'\n",
    "        logit = dropout_layer(logit, use_noise, trng)\n",
    "\n",
    "    logit = get_layer('ff')[1](tparams, logit, options,\n",
    "                               prefix='ff_logit',\n",
    "                               activ='linear')\n",
    "    next_probs = tensor.nnet.softmax(logit)\n",
    "    next_sample = trng.multinomial(pvals=next_probs).argmax(1)\n",
    "\n",
    "    # next word probability\n",
    "    print 'Building f_next...',\n",
    "    inps = [y, ctx, init_state_char, init_state_word]\n",
    "    outs = [next_probs, next_sample, next_state_char, next_state_word]\n",
    "    f_next = theano.function(inps, outs, name='f_next', profile=profile)\n",
    "    print 'Done'\n",
    "\n",
    "    return f_init, f_next\n",
    "\n",
    "\n",
    "def gen_sample(tparams, f_init, f_next, x, options, trng=None,\n",
    "               k=1, maxlen=500, stochastic=True, argmax=False):\n",
    "\n",
    "    # k is the beam size we have\n",
    "    if k > 1:\n",
    "        assert not stochastic, \\\n",
    "            'Beam search does not support stochastic sampling'\n",
    "\n",
    "    sample = []\n",
    "    sample_score = []\n",
    "    if stochastic:\n",
    "        sample_score = 0\n",
    "\n",
    "    live_k = 1\n",
    "    dead_k = 0\n",
    "\n",
    "    hyp_samples = [[]] * live_k\n",
    "    hyp_scores = numpy.zeros(live_k).astype('float32')\n",
    "    hyp_states = []\n",
    "\n",
    "    # get initial state of decoder rnn and encoder context\n",
    "    ret = f_init(x)\n",
    "    next_state_char, next_state_word, ctx0 = ret[0], ret[1], ret[2]\n",
    "    next_w = -1 * numpy.ones((1,)).astype('int64')  # bos indicator\n",
    "\n",
    "    for ii in xrange(maxlen):\n",
    "        ctx = numpy.tile(ctx0, [live_k, 1])\n",
    "        inps = [next_w, ctx, next_state_char, next_state_word]\n",
    "        ret = f_next(*inps)\n",
    "        next_p, next_w, next_state_char, next_state_word = ret[0], ret[1], ret[2], ret[3]\n",
    "        if stochastic:\n",
    "            if argmax:\n",
    "                nw = next_p[0].argmax()\n",
    "            else:\n",
    "                nw = next_w[0]\n",
    "            sample.append(nw)\n",
    "            sample_score += next_p[0, nw]\n",
    "            if nw == 0:\n",
    "                break\n",
    "        else:\n",
    "            cand_scores = hyp_scores[:, None] - numpy.log(next_p)\n",
    "            cand_flat = cand_scores.flatten()\n",
    "            ranks_flat = cand_flat.argsort()[:(k-dead_k)]\n",
    "\n",
    "            voc_size = next_p.shape[1]\n",
    "            trans_indices = ranks_flat / voc_size\n",
    "            word_indices = ranks_flat % voc_size\n",
    "            costs = cand_flat[ranks_flat]\n",
    "\n",
    "            new_hyp_samples = []\n",
    "            new_hyp_scores = numpy.zeros(k-dead_k).astype('float32')\n",
    "            new_hyp_states_char = []\n",
    "            new_hyp_states_word = []\n",
    "\n",
    "            for idx, [ti, wi] in enumerate(zip(trans_indices, word_indices)):\n",
    "                new_hyp_samples.append(hyp_samples[ti]+[wi])\n",
    "                new_hyp_scores[idx] = copy.copy(costs[idx])\n",
    "                new_hyp_states_char.append(copy.copy(next_state_char[ti]))\n",
    "                new_hyp_states_word.append(copy.copy(next_state_word[ti]))\n",
    "\n",
    "            # check the finished samples\n",
    "            new_live_k = 0\n",
    "            hyp_samples = []\n",
    "            hyp_scores = []\n",
    "            hyp_states_char = []\n",
    "            hyp_states_word = []\n",
    "\n",
    "            for idx in xrange(len(new_hyp_samples)):\n",
    "                if new_hyp_samples[idx][-1] == 0:\n",
    "                    sample.append(new_hyp_samples[idx])\n",
    "                    sample_score.append(new_hyp_scores[idx])\n",
    "                    dead_k += 1\n",
    "                else:\n",
    "                    new_live_k += 1\n",
    "                    hyp_samples.append(new_hyp_samples[idx])\n",
    "                    hyp_scores.append(new_hyp_scores[idx])\n",
    "                    hyp_states_char.append(new_hyp_states_char[idx])\n",
    "                    hyp_states_word.append(new_hyp_states_word[idx])\n",
    "            hyp_scores = numpy.array(hyp_scores)\n",
    "            live_k = new_live_k\n",
    "\n",
    "            if new_live_k < 1:\n",
    "                break\n",
    "            if dead_k >= k:\n",
    "                break\n",
    "\n",
    "            next_w = numpy.array([w[-1] for w in hyp_samples])\n",
    "            next_state_char = numpy.array(hyp_states_char)\n",
    "            next_state_word = numpy.array(hyp_states_word)\n",
    "\n",
    "    if not stochastic:\n",
    "        # dump every remaining one\n",
    "        if live_k > 0:\n",
    "            for idx in xrange(live_k):\n",
    "                sample.append(hyp_samples[idx])\n",
    "                sample_score.append(hyp_scores[idx])\n",
    "\n",
    "    return sample, sample_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
